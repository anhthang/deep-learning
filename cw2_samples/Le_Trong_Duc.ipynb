{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Le_Trong_Duc.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WoiKfRdRstLi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"7797a06b-22a0-44c5-bafe-aad2e6b3f3f0"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LoVnyS6Fs1k0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"81dbfe1e-b7aa-4f05-ddc7-46fd2a31594a"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","\n","bs = 100\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='/content/data/', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = datasets.MNIST(root='/content/data/', train=False, transform=transforms.ToTensor(), download=True)\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["9920512it [00:01, 8759729.40it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /content/data/MNIST/raw/train-images-idx3-ubyte.gz to /content/data/MNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/28881 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 136249.36it/s]           \n","  0%|          | 0/1648877 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting /content/data/MNIST/raw/train-labels-idx1-ubyte.gz to /content/data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["1654784it [00:00, 2205547.13it/s]                            \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting /content/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 49960.94it/s]            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting /content/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sah6V3dES4dd","colab_type":"code","colab":{}},"source":["# for batch_idx, (data, _) in enumerate(train_loader):\n","#     x= data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYc9Jd_gUP7p","colab_type":"code","colab":{}},"source":["# save_image(data[1].view(1, 1, 28, 28), '/content/sample_1' + '.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqs7eP61wN_4","colab_type":"code","colab":{}},"source":["class VAE(nn.Module):\n","  def __init__(self, x_dim, h1_dim, h2_dim, z_dim):\n","    super(VAE, self).__init__()\n","    \n","    #encoder layer\n","    self.e_layer1 = nn.Linear(x_dim, h1_dim)\n","    self.e_layer2 = nn.Linear(h1_dim, h2_dim)\n","    \n","    self.mu = nn.Linear(h2_dim, z_dim)\n","    self.log_var = nn.Linear(h2_dim, z_dim)\n","    \n","    #decoder layer\n","    self.d_layer1 = nn.Linear(z_dim, h2_dim)\n","    self.d_layer2 = nn.Linear(h2_dim, h1_dim)\n","    self.d_layer3 = nn.Linear(h1_dim, x_dim)\n","    \n","  def encoder(self, x):\n","    h = F.relu(self.e_layer1(x))\n","    h = F.relu(self.e_layer2(h))\n","    mu = self.mu(h)\n","    log_var = self.log_var(h)\n","    return mu, log_var\n","  \n","  def sampling(self, mu, log_var):\n","    var = torch.exp(0.5*log_var)\n","    eps = torch.randn_like(var)\n","    return eps.mul(var).add_(mu)\n","  \n","  def decoder(self, z):\n","    h = F.relu(self.d_layer1(z))\n","    h = F.relu(self.d_layer2(h))\n","    output = F.sigmoid(self.d_layer3(h))\n","    return output\n","  \n","  def forward(self, input):\n","    mu, log_var = self.encoder(input.view(-1,784))\n","    z = self.sampling(mu, log_var)\n","    output = self.decoder(z)\n","    return output, mu, log_var\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GjUAkAeC4Wp5","colab_type":"code","colab":{}},"source":["vae = VAE(784, 512, 256, 2)\n","# if torch.cuda.is_available():\n","#     vae.cuda()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4MTentz4XK_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"outputId":"12ea893c-1343-4853-df2d-06df48420219"},"source":["vae"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VAE(\n","  (e_layer1): Linear(in_features=784, out_features=512, bias=True)\n","  (e_layer2): Linear(in_features=512, out_features=256, bias=True)\n","  (mu): Linear(in_features=256, out_features=2, bias=True)\n","  (log_var): Linear(in_features=256, out_features=2, bias=True)\n","  (d_layer1): Linear(in_features=2, out_features=256, bias=True)\n","  (d_layer2): Linear(in_features=256, out_features=512, bias=True)\n","  (d_layer3): Linear(in_features=512, out_features=784, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"AxsiMSLL4avR","colab_type":"code","colab":{}},"source":["def loss_function(recon_x, x, mu, log_var):\n","  KL = -0.5*torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","  CE = F.binary_cross_entropy(recon_x, x.view(-1,784), reduction = 'sum')\n","  return KL + CE\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oh8vAFspBaja","colab_type":"code","colab":{}},"source":["optimizer = optim.Adam(vae.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICyZ_ZRLBkRE","colab_type":"code","colab":{}},"source":["def train(epoches):\n","  vae.train()\n","  train_loss = 0\n","  for batch_idx, (data, _) in enumerate(train_loader):\n","    \n","#     if torch.cuda.is_available():\n","#       data.cuda()\n","    \n","    optimizer.zero_grad()\n","    \n","    recon_x, mu, log_var = vae(data)\n","    loss = loss_function(recon_x, data, mu, log_var)\n","    \n","    loss.backward()\n","    train_loss += loss.item()\n","    optimizer.step()\n","    \n","    if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n","  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1g9ztElDB_-","colab_type":"code","colab":{}},"source":["def test():\n","  vae.eval()\n","  test_loss = 0\n","  with torch.no_grad():\n","    for data, _ in test_loader:\n","#       if torch.cuda.is_available():\n","#         data.cuda()\n","      \n","      recon_data, mu, log_var = vae(data)\n","      loss = loss_function(recon_data, data, mu, log_var)\n","      \n","      test_loss += loss\n","      \n","  test_loss /= len(test_loader.dataset)\n","  print('====> Test set loss: {:.4f}'.format(test_loss))      \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFNIhFSIEI3S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"34f67a6b-f706-4b50-b514-8121f1ffd79a"},"source":["for epoch in range(1, 51):\n","    train(epoch)\n","    test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 545.237891\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 193.617324\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 171.796484\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 174.670703\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 170.208633\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 164.590840\n","====> Epoch: 1 Average loss: 179.0982\n","====> Test set loss: 162.1939\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 156.130908\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 157.924141\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 159.876729\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 151.808057\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 158.874111\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 158.268740\n","====> Epoch: 2 Average loss: 157.7264\n","====> Test set loss: 154.8526\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 161.476729\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 160.317002\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 157.707656\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 147.490967\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 159.674170\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 153.928965\n","====> Epoch: 3 Average loss: 152.4037\n","====> Test set loss: 151.2634\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 151.541865\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 155.661465\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 147.441064\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 150.660205\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 151.568672\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 148.390889\n","====> Epoch: 4 Average loss: 149.4655\n","====> Test set loss: 148.4487\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 150.507598\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 145.783740\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 146.379863\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 150.338398\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 149.522139\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 151.095635\n","====> Epoch: 5 Average loss: 147.2814\n","====> Test set loss: 147.3000\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 145.936309\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 144.182988\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 154.975557\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 140.211250\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 155.428262\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 144.408721\n","====> Epoch: 6 Average loss: 145.9066\n","====> Test set loss: 146.0096\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 137.081543\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 147.806318\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 141.790801\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 142.007695\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 144.820400\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 142.625781\n","====> Epoch: 7 Average loss: 144.9006\n","====> Test set loss: 144.8109\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 136.772529\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 142.338838\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 147.376113\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 140.551025\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 151.310986\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 146.889043\n","====> Epoch: 8 Average loss: 144.0874\n","====> Test set loss: 144.4377\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 147.148398\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: 133.653691\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: 147.759883\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: 138.876104\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: 147.246465\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: 144.350371\n","====> Epoch: 9 Average loss: 143.2841\n","====> Test set loss: 143.4729\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 145.528984\n","Train Epoch: 10 [10000/60000 (17%)]\tLoss: 142.915986\n","Train Epoch: 10 [20000/60000 (33%)]\tLoss: 139.874424\n","Train Epoch: 10 [30000/60000 (50%)]\tLoss: 141.227510\n","Train Epoch: 10 [40000/60000 (67%)]\tLoss: 135.991367\n","Train Epoch: 10 [50000/60000 (83%)]\tLoss: 149.670635\n","====> Epoch: 10 Average loss: 142.8444\n","====> Test set loss: 143.5116\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 158.340313\n","Train Epoch: 11 [10000/60000 (17%)]\tLoss: 143.716719\n","Train Epoch: 11 [20000/60000 (33%)]\tLoss: 143.132148\n","Train Epoch: 11 [30000/60000 (50%)]\tLoss: 146.944961\n","Train Epoch: 11 [40000/60000 (67%)]\tLoss: 138.670752\n","Train Epoch: 11 [50000/60000 (83%)]\tLoss: 148.584521\n","====> Epoch: 11 Average loss: 142.1890\n","====> Test set loss: 143.1502\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 144.125273\n","Train Epoch: 12 [10000/60000 (17%)]\tLoss: 129.914863\n","Train Epoch: 12 [20000/60000 (33%)]\tLoss: 142.695273\n","Train Epoch: 12 [30000/60000 (50%)]\tLoss: 151.801084\n","Train Epoch: 12 [40000/60000 (67%)]\tLoss: 142.811084\n","Train Epoch: 12 [50000/60000 (83%)]\tLoss: 139.579668\n","====> Epoch: 12 Average loss: 141.7590\n","====> Test set loss: 142.5605\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 140.711768\n","Train Epoch: 13 [10000/60000 (17%)]\tLoss: 146.172188\n","Train Epoch: 13 [20000/60000 (33%)]\tLoss: 134.661006\n","Train Epoch: 13 [30000/60000 (50%)]\tLoss: 148.831240\n","Train Epoch: 13 [40000/60000 (67%)]\tLoss: 137.234951\n","Train Epoch: 13 [50000/60000 (83%)]\tLoss: 142.804414\n","====> Epoch: 13 Average loss: 141.2291\n","====> Test set loss: 142.3292\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 136.749346\n","Train Epoch: 14 [10000/60000 (17%)]\tLoss: 144.959102\n","Train Epoch: 14 [20000/60000 (33%)]\tLoss: 146.209111\n","Train Epoch: 14 [30000/60000 (50%)]\tLoss: 131.559063\n","Train Epoch: 14 [40000/60000 (67%)]\tLoss: 140.846533\n","Train Epoch: 14 [50000/60000 (83%)]\tLoss: 132.176162\n","====> Epoch: 14 Average loss: 140.9089\n","====> Test set loss: 142.1453\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 136.323525\n","Train Epoch: 15 [10000/60000 (17%)]\tLoss: 142.330518\n","Train Epoch: 15 [20000/60000 (33%)]\tLoss: 138.188320\n","Train Epoch: 15 [30000/60000 (50%)]\tLoss: 146.938984\n","Train Epoch: 15 [40000/60000 (67%)]\tLoss: 137.030986\n","Train Epoch: 15 [50000/60000 (83%)]\tLoss: 144.062705\n","====> Epoch: 15 Average loss: 140.6633\n","====> Test set loss: 141.7944\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 139.703809\n","Train Epoch: 16 [10000/60000 (17%)]\tLoss: 138.943535\n","Train Epoch: 16 [20000/60000 (33%)]\tLoss: 134.324551\n","Train Epoch: 16 [30000/60000 (50%)]\tLoss: 143.827637\n","Train Epoch: 16 [40000/60000 (67%)]\tLoss: 141.211357\n","Train Epoch: 16 [50000/60000 (83%)]\tLoss: 136.004336\n","====> Epoch: 16 Average loss: 140.2532\n","====> Test set loss: 141.7030\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 142.340313\n","Train Epoch: 17 [10000/60000 (17%)]\tLoss: 145.727432\n","Train Epoch: 17 [20000/60000 (33%)]\tLoss: 141.887051\n","Train Epoch: 17 [30000/60000 (50%)]\tLoss: 143.289980\n","Train Epoch: 17 [40000/60000 (67%)]\tLoss: 137.661094\n","Train Epoch: 17 [50000/60000 (83%)]\tLoss: 133.698447\n","====> Epoch: 17 Average loss: 139.9945\n","====> Test set loss: 141.3536\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 139.892705\n","Train Epoch: 18 [10000/60000 (17%)]\tLoss: 143.272129\n","Train Epoch: 18 [20000/60000 (33%)]\tLoss: 136.876045\n","Train Epoch: 18 [30000/60000 (50%)]\tLoss: 133.525586\n","Train Epoch: 18 [40000/60000 (67%)]\tLoss: 142.561914\n","Train Epoch: 18 [50000/60000 (83%)]\tLoss: 124.557744\n","====> Epoch: 18 Average loss: 139.6093\n","====> Test set loss: 141.6457\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 142.217773\n","Train Epoch: 19 [10000/60000 (17%)]\tLoss: 144.607119\n","Train Epoch: 19 [20000/60000 (33%)]\tLoss: 136.937842\n","Train Epoch: 19 [30000/60000 (50%)]\tLoss: 147.382559\n","Train Epoch: 19 [40000/60000 (67%)]\tLoss: 142.816846\n","Train Epoch: 19 [50000/60000 (83%)]\tLoss: 137.791084\n","====> Epoch: 19 Average loss: 139.3774\n","====> Test set loss: 140.8668\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 145.763506\n","Train Epoch: 20 [10000/60000 (17%)]\tLoss: 136.524492\n","Train Epoch: 20 [20000/60000 (33%)]\tLoss: 133.050371\n","Train Epoch: 20 [30000/60000 (50%)]\tLoss: 140.767207\n","Train Epoch: 20 [40000/60000 (67%)]\tLoss: 140.211982\n","Train Epoch: 20 [50000/60000 (83%)]\tLoss: 131.266035\n","====> Epoch: 20 Average loss: 139.0561\n","====> Test set loss: 140.7457\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 130.799678\n","Train Epoch: 21 [10000/60000 (17%)]\tLoss: 135.828096\n","Train Epoch: 21 [20000/60000 (33%)]\tLoss: 134.539648\n","Train Epoch: 21 [30000/60000 (50%)]\tLoss: 135.776982\n","Train Epoch: 21 [40000/60000 (67%)]\tLoss: 131.670234\n","Train Epoch: 21 [50000/60000 (83%)]\tLoss: 130.241729\n","====> Epoch: 21 Average loss: 138.8620\n","====> Test set loss: 140.7921\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 136.829941\n","Train Epoch: 22 [10000/60000 (17%)]\tLoss: 144.241387\n","Train Epoch: 22 [20000/60000 (33%)]\tLoss: 135.945391\n","Train Epoch: 22 [30000/60000 (50%)]\tLoss: 136.056582\n","Train Epoch: 22 [40000/60000 (67%)]\tLoss: 134.157070\n","Train Epoch: 22 [50000/60000 (83%)]\tLoss: 136.259912\n","====> Epoch: 22 Average loss: 138.4943\n","====> Test set loss: 140.0508\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 142.897441\n","Train Epoch: 23 [10000/60000 (17%)]\tLoss: 138.439404\n","Train Epoch: 23 [20000/60000 (33%)]\tLoss: 140.516836\n","Train Epoch: 23 [30000/60000 (50%)]\tLoss: 132.999072\n","Train Epoch: 23 [40000/60000 (67%)]\tLoss: 140.268760\n","Train Epoch: 23 [50000/60000 (83%)]\tLoss: 141.516387\n","====> Epoch: 23 Average loss: 138.2579\n","====> Test set loss: 140.2189\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 131.493779\n","Train Epoch: 24 [10000/60000 (17%)]\tLoss: 136.507236\n","Train Epoch: 24 [20000/60000 (33%)]\tLoss: 135.372041\n","Train Epoch: 24 [30000/60000 (50%)]\tLoss: 142.515605\n","Train Epoch: 24 [40000/60000 (67%)]\tLoss: 133.895654\n","Train Epoch: 24 [50000/60000 (83%)]\tLoss: 138.341777\n","====> Epoch: 24 Average loss: 138.2486\n","====> Test set loss: 139.7092\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 137.380293\n","Train Epoch: 25 [10000/60000 (17%)]\tLoss: 144.034268\n","Train Epoch: 25 [20000/60000 (33%)]\tLoss: 145.014375\n","Train Epoch: 25 [30000/60000 (50%)]\tLoss: 133.079814\n","Train Epoch: 25 [40000/60000 (67%)]\tLoss: 136.980654\n","Train Epoch: 25 [50000/60000 (83%)]\tLoss: 141.910791\n","====> Epoch: 25 Average loss: 138.0250\n","====> Test set loss: 140.6658\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 135.337305\n","Train Epoch: 26 [10000/60000 (17%)]\tLoss: 139.130879\n","Train Epoch: 26 [20000/60000 (33%)]\tLoss: 140.563105\n","Train Epoch: 26 [30000/60000 (50%)]\tLoss: 132.778906\n","Train Epoch: 26 [40000/60000 (67%)]\tLoss: 140.621133\n","Train Epoch: 26 [50000/60000 (83%)]\tLoss: 136.042852\n","====> Epoch: 26 Average loss: 137.9475\n","====> Test set loss: 139.8688\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 141.046836\n","Train Epoch: 27 [10000/60000 (17%)]\tLoss: 140.852002\n","Train Epoch: 27 [20000/60000 (33%)]\tLoss: 138.401504\n","Train Epoch: 27 [30000/60000 (50%)]\tLoss: 139.778994\n","Train Epoch: 27 [40000/60000 (67%)]\tLoss: 139.221025\n","Train Epoch: 27 [50000/60000 (83%)]\tLoss: 127.324170\n","====> Epoch: 27 Average loss: 137.7418\n","====> Test set loss: 140.0676\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 135.760059\n","Train Epoch: 28 [10000/60000 (17%)]\tLoss: 135.352500\n","Train Epoch: 28 [20000/60000 (33%)]\tLoss: 147.529492\n","Train Epoch: 28 [30000/60000 (50%)]\tLoss: 134.029844\n","Train Epoch: 28 [40000/60000 (67%)]\tLoss: 131.007529\n","Train Epoch: 28 [50000/60000 (83%)]\tLoss: 135.867627\n","====> Epoch: 28 Average loss: 137.3577\n","====> Test set loss: 139.4770\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 136.027998\n","Train Epoch: 29 [10000/60000 (17%)]\tLoss: 137.397529\n","Train Epoch: 29 [20000/60000 (33%)]\tLoss: 134.448428\n","Train Epoch: 29 [30000/60000 (50%)]\tLoss: 137.142988\n","Train Epoch: 29 [40000/60000 (67%)]\tLoss: 138.690537\n","Train Epoch: 29 [50000/60000 (83%)]\tLoss: 133.544248\n","====> Epoch: 29 Average loss: 137.1440\n","====> Test set loss: 139.8344\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 135.313906\n","Train Epoch: 30 [10000/60000 (17%)]\tLoss: 142.385879\n","Train Epoch: 30 [20000/60000 (33%)]\tLoss: 138.286865\n","Train Epoch: 30 [30000/60000 (50%)]\tLoss: 133.933193\n","Train Epoch: 30 [40000/60000 (67%)]\tLoss: 138.811074\n","Train Epoch: 30 [50000/60000 (83%)]\tLoss: 138.409102\n","====> Epoch: 30 Average loss: 137.1842\n","====> Test set loss: 139.1111\n","Train Epoch: 31 [0/60000 (0%)]\tLoss: 138.246182\n","Train Epoch: 31 [10000/60000 (17%)]\tLoss: 135.328682\n","Train Epoch: 31 [20000/60000 (33%)]\tLoss: 139.117676\n","Train Epoch: 31 [30000/60000 (50%)]\tLoss: 136.148398\n","Train Epoch: 31 [40000/60000 (67%)]\tLoss: 138.795938\n","Train Epoch: 31 [50000/60000 (83%)]\tLoss: 137.160469\n","====> Epoch: 31 Average loss: 136.7454\n","====> Test set loss: 139.3718\n","Train Epoch: 32 [0/60000 (0%)]\tLoss: 140.316338\n","Train Epoch: 32 [10000/60000 (17%)]\tLoss: 141.386201\n","Train Epoch: 32 [20000/60000 (33%)]\tLoss: 138.114580\n","Train Epoch: 32 [30000/60000 (50%)]\tLoss: 136.013936\n","Train Epoch: 32 [40000/60000 (67%)]\tLoss: 133.105596\n","Train Epoch: 32 [50000/60000 (83%)]\tLoss: 134.166787\n","====> Epoch: 32 Average loss: 136.7397\n","====> Test set loss: 139.2924\n","Train Epoch: 33 [0/60000 (0%)]\tLoss: 134.973672\n","Train Epoch: 33 [10000/60000 (17%)]\tLoss: 137.355000\n","Train Epoch: 33 [20000/60000 (33%)]\tLoss: 137.471279\n","Train Epoch: 33 [30000/60000 (50%)]\tLoss: 133.467158\n","Train Epoch: 33 [40000/60000 (67%)]\tLoss: 144.980791\n","Train Epoch: 33 [50000/60000 (83%)]\tLoss: 140.831855\n","====> Epoch: 33 Average loss: 136.7809\n","====> Test set loss: 139.2507\n","Train Epoch: 34 [0/60000 (0%)]\tLoss: 148.300449\n","Train Epoch: 34 [10000/60000 (17%)]\tLoss: 137.543037\n","Train Epoch: 34 [20000/60000 (33%)]\tLoss: 135.841494\n","Train Epoch: 34 [30000/60000 (50%)]\tLoss: 130.926289\n","Train Epoch: 34 [40000/60000 (67%)]\tLoss: 133.150830\n","Train Epoch: 34 [50000/60000 (83%)]\tLoss: 140.557441\n","====> Epoch: 34 Average loss: 136.6203\n","====> Test set loss: 138.9711\n","Train Epoch: 35 [0/60000 (0%)]\tLoss: 135.511426\n","Train Epoch: 35 [10000/60000 (17%)]\tLoss: 141.884385\n","Train Epoch: 35 [20000/60000 (33%)]\tLoss: 129.442393\n","Train Epoch: 35 [30000/60000 (50%)]\tLoss: 132.771943\n","Train Epoch: 35 [40000/60000 (67%)]\tLoss: 135.040977\n","Train Epoch: 35 [50000/60000 (83%)]\tLoss: 139.219160\n","====> Epoch: 35 Average loss: 136.4191\n","====> Test set loss: 138.9193\n","Train Epoch: 36 [0/60000 (0%)]\tLoss: 129.062607\n","Train Epoch: 36 [10000/60000 (17%)]\tLoss: 132.103350\n","Train Epoch: 36 [20000/60000 (33%)]\tLoss: 132.276025\n","Train Epoch: 36 [30000/60000 (50%)]\tLoss: 132.585547\n","Train Epoch: 36 [40000/60000 (67%)]\tLoss: 134.494570\n","Train Epoch: 36 [50000/60000 (83%)]\tLoss: 135.171572\n","====> Epoch: 36 Average loss: 136.2678\n","====> Test set loss: 138.8778\n","Train Epoch: 37 [0/60000 (0%)]\tLoss: 138.931328\n","Train Epoch: 37 [10000/60000 (17%)]\tLoss: 137.927197\n","Train Epoch: 37 [20000/60000 (33%)]\tLoss: 133.954961\n","Train Epoch: 37 [30000/60000 (50%)]\tLoss: 133.508223\n","Train Epoch: 37 [40000/60000 (67%)]\tLoss: 137.783525\n","Train Epoch: 37 [50000/60000 (83%)]\tLoss: 128.611055\n","====> Epoch: 37 Average loss: 135.9821\n","====> Test set loss: 139.1838\n","Train Epoch: 38 [0/60000 (0%)]\tLoss: 141.338340\n","Train Epoch: 38 [10000/60000 (17%)]\tLoss: 138.495918\n","Train Epoch: 38 [20000/60000 (33%)]\tLoss: 134.782285\n","Train Epoch: 38 [30000/60000 (50%)]\tLoss: 130.855215\n","Train Epoch: 38 [40000/60000 (67%)]\tLoss: 137.868965\n","Train Epoch: 38 [50000/60000 (83%)]\tLoss: 137.299570\n","====> Epoch: 38 Average loss: 136.4145\n","====> Test set loss: 138.7889\n","Train Epoch: 39 [0/60000 (0%)]\tLoss: 144.257861\n","Train Epoch: 39 [10000/60000 (17%)]\tLoss: 139.864248\n","Train Epoch: 39 [20000/60000 (33%)]\tLoss: 139.048525\n","Train Epoch: 39 [30000/60000 (50%)]\tLoss: 132.840000\n","Train Epoch: 39 [40000/60000 (67%)]\tLoss: 137.138633\n","Train Epoch: 39 [50000/60000 (83%)]\tLoss: 136.328799\n","====> Epoch: 39 Average loss: 136.3898\n","====> Test set loss: 138.8951\n","Train Epoch: 40 [0/60000 (0%)]\tLoss: 125.108066\n","Train Epoch: 40 [10000/60000 (17%)]\tLoss: 139.927568\n","Train Epoch: 40 [20000/60000 (33%)]\tLoss: 141.247207\n","Train Epoch: 40 [30000/60000 (50%)]\tLoss: 138.057422\n","Train Epoch: 40 [40000/60000 (67%)]\tLoss: 140.166787\n","Train Epoch: 40 [50000/60000 (83%)]\tLoss: 133.226260\n","====> Epoch: 40 Average loss: 135.9888\n","====> Test set loss: 138.9158\n","Train Epoch: 41 [0/60000 (0%)]\tLoss: 132.283066\n","Train Epoch: 41 [10000/60000 (17%)]\tLoss: 129.785137\n","Train Epoch: 41 [20000/60000 (33%)]\tLoss: 136.301758\n","Train Epoch: 41 [30000/60000 (50%)]\tLoss: 139.990586\n","Train Epoch: 41 [40000/60000 (67%)]\tLoss: 143.592314\n","Train Epoch: 41 [50000/60000 (83%)]\tLoss: 136.034004\n","====> Epoch: 41 Average loss: 136.1217\n","====> Test set loss: 140.0089\n","Train Epoch: 42 [0/60000 (0%)]\tLoss: 137.901035\n","Train Epoch: 42 [10000/60000 (17%)]\tLoss: 139.633320\n","Train Epoch: 42 [20000/60000 (33%)]\tLoss: 145.394902\n","Train Epoch: 42 [30000/60000 (50%)]\tLoss: 129.907773\n","Train Epoch: 42 [40000/60000 (67%)]\tLoss: 132.019980\n","Train Epoch: 42 [50000/60000 (83%)]\tLoss: 141.978691\n","====> Epoch: 42 Average loss: 135.9747\n","====> Test set loss: 138.9176\n","Train Epoch: 43 [0/60000 (0%)]\tLoss: 137.639570\n","Train Epoch: 43 [10000/60000 (17%)]\tLoss: 140.252383\n","Train Epoch: 43 [20000/60000 (33%)]\tLoss: 134.024775\n","Train Epoch: 43 [30000/60000 (50%)]\tLoss: 134.978594\n","Train Epoch: 43 [40000/60000 (67%)]\tLoss: 138.510879\n","Train Epoch: 43 [50000/60000 (83%)]\tLoss: 138.189971\n","====> Epoch: 43 Average loss: 136.0638\n","====> Test set loss: 139.5208\n","Train Epoch: 44 [0/60000 (0%)]\tLoss: 128.637861\n","Train Epoch: 44 [10000/60000 (17%)]\tLoss: 143.748193\n","Train Epoch: 44 [20000/60000 (33%)]\tLoss: 134.698311\n","Train Epoch: 44 [30000/60000 (50%)]\tLoss: 129.982139\n","Train Epoch: 44 [40000/60000 (67%)]\tLoss: 134.895566\n","Train Epoch: 44 [50000/60000 (83%)]\tLoss: 135.360723\n","====> Epoch: 44 Average loss: 135.6029\n","====> Test set loss: 139.0603\n","Train Epoch: 45 [0/60000 (0%)]\tLoss: 138.957754\n","Train Epoch: 45 [10000/60000 (17%)]\tLoss: 133.481953\n","Train Epoch: 45 [20000/60000 (33%)]\tLoss: 131.315752\n","Train Epoch: 45 [30000/60000 (50%)]\tLoss: 133.579473\n","Train Epoch: 45 [40000/60000 (67%)]\tLoss: 131.850508\n","Train Epoch: 45 [50000/60000 (83%)]\tLoss: 134.220303\n","====> Epoch: 45 Average loss: 135.8148\n","====> Test set loss: 139.0494\n","Train Epoch: 46 [0/60000 (0%)]\tLoss: 133.771641\n","Train Epoch: 46 [10000/60000 (17%)]\tLoss: 134.677021\n","Train Epoch: 46 [20000/60000 (33%)]\tLoss: 133.407275\n","Train Epoch: 46 [30000/60000 (50%)]\tLoss: 135.058652\n","Train Epoch: 46 [40000/60000 (67%)]\tLoss: 133.455449\n","Train Epoch: 46 [50000/60000 (83%)]\tLoss: 130.674492\n","====> Epoch: 46 Average loss: 135.6034\n","====> Test set loss: 138.7761\n","Train Epoch: 47 [0/60000 (0%)]\tLoss: 140.799131\n","Train Epoch: 47 [10000/60000 (17%)]\tLoss: 132.304434\n","Train Epoch: 47 [20000/60000 (33%)]\tLoss: 145.015635\n","Train Epoch: 47 [30000/60000 (50%)]\tLoss: 127.350752\n","Train Epoch: 47 [40000/60000 (67%)]\tLoss: 142.961094\n","Train Epoch: 47 [50000/60000 (83%)]\tLoss: 135.312061\n","====> Epoch: 47 Average loss: 135.5744\n","====> Test set loss: 138.6835\n","Train Epoch: 48 [0/60000 (0%)]\tLoss: 128.459492\n","Train Epoch: 48 [10000/60000 (17%)]\tLoss: 141.995859\n","Train Epoch: 48 [20000/60000 (33%)]\tLoss: 128.025615\n","Train Epoch: 48 [30000/60000 (50%)]\tLoss: 131.881055\n","Train Epoch: 48 [40000/60000 (67%)]\tLoss: 132.631387\n","Train Epoch: 48 [50000/60000 (83%)]\tLoss: 143.927305\n","====> Epoch: 48 Average loss: 135.4259\n","====> Test set loss: 138.5798\n","Train Epoch: 49 [0/60000 (0%)]\tLoss: 130.394033\n","Train Epoch: 49 [10000/60000 (17%)]\tLoss: 136.383623\n","Train Epoch: 49 [20000/60000 (33%)]\tLoss: 140.946758\n","Train Epoch: 49 [30000/60000 (50%)]\tLoss: 128.642939\n","Train Epoch: 49 [40000/60000 (67%)]\tLoss: 135.538857\n","Train Epoch: 49 [50000/60000 (83%)]\tLoss: 131.654775\n","====> Epoch: 49 Average loss: 135.1035\n","====> Test set loss: 138.6163\n","Train Epoch: 50 [0/60000 (0%)]\tLoss: 133.238545\n","Train Epoch: 50 [10000/60000 (17%)]\tLoss: 133.820859\n","Train Epoch: 50 [20000/60000 (33%)]\tLoss: 133.501563\n","Train Epoch: 50 [30000/60000 (50%)]\tLoss: 135.022754\n","Train Epoch: 50 [40000/60000 (67%)]\tLoss: 133.042578\n","Train Epoch: 50 [50000/60000 (83%)]\tLoss: 131.947529\n","====> Epoch: 50 Average loss: 134.9456\n","====> Test set loss: 138.6074\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"koYZVsatEKej","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"00458287-4a20-4d29-f8a6-2d42504a4970"},"source":["\n","with torch.no_grad():\n","    z = torch.randn(64, 2)\n","    print(z)\n","    sample = vae.decoder(z)\n","    save_image(sample.view(64, 1, 28, 28), 'drive/My Drive/activelaerning/homework2/sample_image1' + '.png')\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-1.8327,  0.6776],\n","        [ 0.1008,  0.7682],\n","        [-0.9421, -1.0095],\n","        [-0.0610, -1.1693],\n","        [ 0.1354, -1.2014],\n","        [-0.8841,  0.0337],\n","        [ 2.2786,  2.2434],\n","        [-0.2016, -0.5290],\n","        [-0.0047, -0.0970],\n","        [-0.0092,  0.2998],\n","        [ 1.2681,  0.2654],\n","        [ 0.8999,  0.1292],\n","        [ 1.9096,  0.4173],\n","        [ 0.7496, -1.5915],\n","        [-0.1208, -0.4132],\n","        [-0.3170, -0.0283],\n","        [-0.2089,  1.1713],\n","        [ 0.4444, -0.3043],\n","        [ 1.5079,  0.6345],\n","        [ 0.2383,  1.8588],\n","        [ 0.2820, -0.0825],\n","        [ 0.8087, -0.7634],\n","        [-1.4636, -0.2152],\n","        [-1.7808,  1.3852],\n","        [ 0.4493, -1.3441],\n","        [ 0.5636, -0.8082],\n","        [ 1.4323, -1.9749],\n","        [ 0.2321, -0.0047],\n","        [-0.2981, -0.3689],\n","        [-1.1723, -0.3658],\n","        [ 0.1300, -0.9767],\n","        [ 0.3471,  0.2877],\n","        [ 1.8667,  1.0478],\n","        [-3.4196, -1.8831],\n","        [-0.5876,  0.4294],\n","        [ 1.6508, -0.5145],\n","        [ 0.8624, -1.7989],\n","        [ 0.7197, -0.4337],\n","        [ 0.9718, -0.3488],\n","        [-0.4806, -0.4111],\n","        [ 0.0559,  0.7931],\n","        [-0.3098, -1.4891],\n","        [ 0.1989, -0.5738],\n","        [-0.8905,  0.8217],\n","        [-1.1038,  0.7348],\n","        [-2.9875, -0.1435],\n","        [ 0.4650,  0.3685],\n","        [-2.0294,  1.2216],\n","        [ 0.7569, -0.4922],\n","        [ 0.8193,  1.3264],\n","        [-0.5316,  2.1277],\n","        [-0.7845, -0.8089],\n","        [ 1.4560, -1.3980],\n","        [-0.6014, -1.2018],\n","        [ 1.5943,  2.5499],\n","        [-1.1048, -0.2878],\n","        [ 0.9024, -1.0022],\n","        [ 0.1538,  0.0531],\n","        [-1.2503, -0.1680],\n","        [ 0.1489,  1.2189],\n","        [-1.7110,  0.3569],\n","        [ 0.5790, -0.9630],\n","        [-1.0696,  1.3826],\n","        [ 0.7475, -0.5014]])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YaoGZNFnNyW5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"e39cdea3-0505-4fda-867a-6cc5085424bc"},"source":["torch.save(vae,\"drive/My Drive/activelaerning/homework2/model.h5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type VAE. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"O9BPDdP4ODJZ","colab_type":"code","colab":{}},"source":["the_model = torch.load(\"drive/My Drive/activelaerning/homework2/model.h5\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aB4Frpjkj_IE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"33b1e8cb-a5d0-42a8-a85f-839f15ca452d"},"source":["\n","with torch.no_grad():\n","    z = torch.randn(64, 2)\n","#     print(z)\n","    sample = the_model.decoder(z)\n","    save_image(sample.view(64, 1, 28, 28), 'sample_image1' + '.png')\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XI5lFu5wkaTb","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}