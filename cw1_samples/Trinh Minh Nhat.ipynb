{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trinh Minh Nhat.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"u6iTa03YkfOG","colab_type":"code","outputId":"239fd9c8-f48f-40e4-c278-7dab1d64d3e1","executionInfo":{"status":"ok","timestamp":1569592265825,"user_tz":-420,"elapsed":2875,"user":{"displayName":"Nhat trinh minh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeQT6iJxnWEHCPvudElEDL8bH4i6qDNE0jGGMkQA=s64","userId":"04755155104375278498"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["from __future__ import print_function\n","import keras\n","from keras.models import load_model\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten,BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n","import os\n","from keras.callbacks import ModelCheckpoint\n","from keras.regularizers import l2\n","batch_size = 64\n","num_classes = 10\n","epochs = 100\n","data_augmentation = True\n","num_predictions = 20\n","save_dir = os.path.join(os.getcwd(), 'saved_models')\n","model_name = 'keras_cifar10_trained_model.h5'\n","# The data, split between train and test sets:\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_val = x_train[45000:50000,:,:,:]\n","y_val = y_train[45000:50000,:]\n","\n","x_train = x_train[:45000,:,:,:]\n","y_train = y_train[:45000,:]\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_val.shape[0], 'val samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# Convert class vectors to binary class matrices.\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","y_val = keras.utils.to_categorical(y_val, num_classes)\n","model = Sequential()\n","\n","model.add(Conv2D(96, (3, 3),padding='same', input_shape=x_train.shape[1:]))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(96, (3, 3),padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(192, (3, 3),padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(192, (3, 3),padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Dropout(0.2))\n","\n","\n","model.add(Conv2D(384, (3, 3),padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(384, (3, 3),padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n","model.add(Dropout(0.2))\n","\n","model.add(GlobalAveragePooling2D())\n","\n","model.add(Dropout(0.2))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","opt = keras.optimizers.Adadelta()\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_val = x_val.astype('float32')\n","x_train /= 255\n","x_val /= 255\n","x_test /= 255\n","\n","mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","callbacks_list = [mc]\n","\n","\n","\n","    "],"execution_count":5,"outputs":[{"output_type":"stream","text":["x_train shape: (45000, 32, 32, 3)\n","45000 train samples\n","5000 val samples\n","10000 test samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2OH9rWSs9UvH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9f602e66-99e8-4dfe-fc42-429af1d61ea8","executionInfo":{"status":"ok","timestamp":1569598299026,"user_tz":-420,"elapsed":6018281,"user":{"displayName":"Nhat trinh minh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeQT6iJxnWEHCPvudElEDL8bH4i6qDNE0jGGMkQA=s64","userId":"04755155104375278498"}}},"source":["if not data_augmentation:\n","    print('Not using data augmentation.')\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_data=(x_val, y_val),callbacks=callbacks_list,\n","              shuffle=True)\n","else:\n","    print('Using real-time data augmentation.')\n","    # This will do preprocessing and realtime data augmentation:\n","    datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n","        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n","        # randomly shift images horizontally (fraction of total width)\n","        width_shift_range=0.1,\n","        # randomly shift images vertically (fraction of total height)\n","        height_shift_range=0.1,\n","        shear_range=0.,  # set range for random shear\n","        zoom_range=0.,  # set range for random zoom\n","        channel_shift_range=0.,  # set range for random channel shifts\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='nearest',\n","        cval=0.,  # value used for fill_mode = \"constant\"\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=False,  # randomly flip images\n","        # set rescaling factor (applied before any other transformation)\n","        rescale=None,\n","        # set function that will be applied on each input\n","        preprocessing_function=None,\n","        # image data format, either \"channels_first\" or \"channels_last\"\n","        data_format=None,\n","        # fraction of images reserved for validation (strictly between 0 and 1)\n","        validation_split=0.0)\n","\n","    # Compute quantities required for feature-wise normalization\n","    # (std, mean, and principal components if ZCA whitening is applied).\n","    datagen.fit(x_train)\n","\n","    # Fit the model on the batches generated by datagen.flow().\n","    model.fit_generator(datagen.flow(x_train, y_train,\n","                                     batch_size=batch_size),\n","                        epochs=epochs,\n","                        validation_data=(x_val, y_val),callbacks=callbacks_list,\n","                        workers=4)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using real-time data augmentation.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Epoch 1/100\n","704/704 [==============================] - 66s 93ms/step - loss: 1.4649 - acc: 0.4727 - val_loss: 1.4756 - val_acc: 0.4986\n","\n","Epoch 00001: val_acc improved from -inf to 0.49860, saving model to best_model.h5\n","Epoch 2/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.9906 - acc: 0.6483 - val_loss: 1.1783 - val_acc: 0.6432\n","\n","Epoch 00002: val_acc improved from 0.49860 to 0.64320, saving model to best_model.h5\n","Epoch 3/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.8126 - acc: 0.7162 - val_loss: 1.0711 - val_acc: 0.6420\n","\n","Epoch 00003: val_acc did not improve from 0.64320\n","Epoch 4/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.7163 - acc: 0.7516 - val_loss: 0.7604 - val_acc: 0.7320\n","\n","Epoch 00004: val_acc improved from 0.64320 to 0.73200, saving model to best_model.h5\n","Epoch 5/100\n","704/704 [==============================] - 61s 86ms/step - loss: 0.6343 - acc: 0.7790 - val_loss: 0.7663 - val_acc: 0.7518\n","\n","Epoch 00005: val_acc improved from 0.73200 to 0.75180, saving model to best_model.h5\n","Epoch 6/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.5849 - acc: 0.7989 - val_loss: 0.6213 - val_acc: 0.7932\n","\n","Epoch 00006: val_acc improved from 0.75180 to 0.79320, saving model to best_model.h5\n","Epoch 7/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.5393 - acc: 0.8145 - val_loss: 1.3513 - val_acc: 0.6320\n","\n","Epoch 00007: val_acc did not improve from 0.79320\n","Epoch 8/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.5005 - acc: 0.8289 - val_loss: 0.6613 - val_acc: 0.7834\n","\n","Epoch 00008: val_acc did not improve from 0.79320\n","Epoch 9/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.4658 - acc: 0.8398 - val_loss: 0.6788 - val_acc: 0.7812\n","\n","Epoch 00009: val_acc did not improve from 0.79320\n","Epoch 10/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.4387 - acc: 0.8488 - val_loss: 0.5688 - val_acc: 0.8080\n","\n","Epoch 00010: val_acc improved from 0.79320 to 0.80800, saving model to best_model.h5\n","Epoch 11/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.4083 - acc: 0.8596 - val_loss: 0.5407 - val_acc: 0.8186\n","\n","Epoch 00011: val_acc improved from 0.80800 to 0.81860, saving model to best_model.h5\n","Epoch 12/100\n","704/704 [==============================] - 62s 88ms/step - loss: 0.3863 - acc: 0.8660 - val_loss: 0.5292 - val_acc: 0.8312\n","\n","Epoch 00012: val_acc improved from 0.81860 to 0.83120, saving model to best_model.h5\n","Epoch 13/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.3724 - acc: 0.8710 - val_loss: 0.5349 - val_acc: 0.8288\n","\n","Epoch 00013: val_acc did not improve from 0.83120\n","Epoch 14/100\n","704/704 [==============================] - 59s 84ms/step - loss: 0.3489 - acc: 0.8805 - val_loss: 0.4417 - val_acc: 0.8512\n","\n","Epoch 00014: val_acc improved from 0.83120 to 0.85120, saving model to best_model.h5\n","Epoch 15/100\n","704/704 [==============================] - 59s 84ms/step - loss: 0.3286 - acc: 0.8873 - val_loss: 0.4013 - val_acc: 0.8696\n","\n","Epoch 00015: val_acc improved from 0.85120 to 0.86960, saving model to best_model.h5\n","Epoch 16/100\n","704/704 [==============================] - 59s 84ms/step - loss: 0.3147 - acc: 0.8911 - val_loss: 0.5253 - val_acc: 0.8362\n","\n","Epoch 00016: val_acc did not improve from 0.86960\n","Epoch 17/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2980 - acc: 0.8959 - val_loss: 0.4169 - val_acc: 0.8618\n","\n","Epoch 00017: val_acc did not improve from 0.86960\n","Epoch 18/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.2867 - acc: 0.9008 - val_loss: 0.4332 - val_acc: 0.8642\n","\n","Epoch 00018: val_acc did not improve from 0.86960\n","Epoch 19/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2717 - acc: 0.9044 - val_loss: 0.4921 - val_acc: 0.8566\n","\n","Epoch 00019: val_acc did not improve from 0.86960\n","Epoch 20/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2603 - acc: 0.9103 - val_loss: 0.4022 - val_acc: 0.8696\n","\n","Epoch 00020: val_acc did not improve from 0.86960\n","Epoch 21/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2520 - acc: 0.9126 - val_loss: 0.5332 - val_acc: 0.8476\n","\n","Epoch 00021: val_acc did not improve from 0.86960\n","Epoch 22/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2344 - acc: 0.9188 - val_loss: 0.3730 - val_acc: 0.8802\n","\n","Epoch 00022: val_acc improved from 0.86960 to 0.88020, saving model to best_model.h5\n","Epoch 23/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.2324 - acc: 0.9181 - val_loss: 0.3985 - val_acc: 0.8808\n","\n","Epoch 00023: val_acc improved from 0.88020 to 0.88080, saving model to best_model.h5\n","Epoch 24/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2186 - acc: 0.9240 - val_loss: 0.4178 - val_acc: 0.8748\n","\n","Epoch 00024: val_acc did not improve from 0.88080\n","Epoch 25/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2116 - acc: 0.9264 - val_loss: 0.4245 - val_acc: 0.8744\n","\n","Epoch 00025: val_acc did not improve from 0.88080\n","Epoch 26/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.2057 - acc: 0.9278 - val_loss: 0.3944 - val_acc: 0.8836\n","\n","Epoch 00026: val_acc improved from 0.88080 to 0.88360, saving model to best_model.h5\n","Epoch 27/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.1928 - acc: 0.9316 - val_loss: 0.4742 - val_acc: 0.8670\n","\n","Epoch 00027: val_acc did not improve from 0.88360\n","Epoch 28/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1870 - acc: 0.9350 - val_loss: 0.3614 - val_acc: 0.8886\n","\n","Epoch 00028: val_acc improved from 0.88360 to 0.88860, saving model to best_model.h5\n","Epoch 29/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1816 - acc: 0.9367 - val_loss: 0.3932 - val_acc: 0.8846\n","\n","Epoch 00029: val_acc did not improve from 0.88860\n","Epoch 30/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1714 - acc: 0.9391 - val_loss: 0.3256 - val_acc: 0.8982\n","\n","Epoch 00030: val_acc improved from 0.88860 to 0.89820, saving model to best_model.h5\n","Epoch 31/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1675 - acc: 0.9409 - val_loss: 0.3879 - val_acc: 0.8846\n","\n","Epoch 00031: val_acc did not improve from 0.89820\n","Epoch 32/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.1631 - acc: 0.9429 - val_loss: 0.4656 - val_acc: 0.8734\n","\n","Epoch 00032: val_acc did not improve from 0.89820\n","Epoch 33/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.1486 - acc: 0.9480 - val_loss: 0.4256 - val_acc: 0.8830\n","\n","Epoch 00033: val_acc did not improve from 0.89820\n","Epoch 34/100\n","704/704 [==============================] - 60s 86ms/step - loss: 0.1512 - acc: 0.9473 - val_loss: 0.4099 - val_acc: 0.8938\n","\n","Epoch 00034: val_acc did not improve from 0.89820\n","Epoch 35/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1434 - acc: 0.9503 - val_loss: 0.4455 - val_acc: 0.8848\n","\n","Epoch 00035: val_acc did not improve from 0.89820\n","Epoch 36/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1433 - acc: 0.9492 - val_loss: 0.4033 - val_acc: 0.8942\n","\n","Epoch 00036: val_acc did not improve from 0.89820\n","Epoch 37/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1383 - acc: 0.9524 - val_loss: 0.4087 - val_acc: 0.8926\n","\n","Epoch 00037: val_acc did not improve from 0.89820\n","Epoch 38/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1314 - acc: 0.9544 - val_loss: 0.3860 - val_acc: 0.8904\n","\n","Epoch 00038: val_acc did not improve from 0.89820\n","Epoch 39/100\n","704/704 [==============================] - 59s 84ms/step - loss: 0.1294 - acc: 0.9539 - val_loss: 0.3916 - val_acc: 0.8968\n","\n","Epoch 00039: val_acc did not improve from 0.89820\n","Epoch 40/100\n","704/704 [==============================] - 59s 84ms/step - loss: 0.1208 - acc: 0.9575 - val_loss: 0.4448 - val_acc: 0.8876\n","\n","Epoch 00040: val_acc did not improve from 0.89820\n","Epoch 41/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1186 - acc: 0.9575 - val_loss: 0.4250 - val_acc: 0.8848\n","\n","Epoch 00041: val_acc did not improve from 0.89820\n","Epoch 42/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1147 - acc: 0.9591 - val_loss: 0.3760 - val_acc: 0.8956\n","\n","Epoch 00042: val_acc did not improve from 0.89820\n","Epoch 43/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1155 - acc: 0.9591 - val_loss: 0.4144 - val_acc: 0.8882\n","\n","Epoch 00043: val_acc did not improve from 0.89820\n","Epoch 44/100\n","704/704 [==============================] - 59s 85ms/step - loss: 0.1108 - acc: 0.9605 - val_loss: 0.3410 - val_acc: 0.9056\n","\n","Epoch 00044: val_acc improved from 0.89820 to 0.90560, saving model to best_model.h5\n","Epoch 45/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1085 - acc: 0.9607 - val_loss: 0.4137 - val_acc: 0.8950\n","\n","Epoch 00045: val_acc did not improve from 0.90560\n","Epoch 46/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1040 - acc: 0.9629 - val_loss: 0.4237 - val_acc: 0.8866\n","\n","Epoch 00046: val_acc did not improve from 0.90560\n","Epoch 47/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.1009 - acc: 0.9644 - val_loss: 0.3880 - val_acc: 0.9002\n","\n","Epoch 00047: val_acc did not improve from 0.90560\n","Epoch 48/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0990 - acc: 0.9644 - val_loss: 0.4497 - val_acc: 0.8868\n","\n","Epoch 00048: val_acc did not improve from 0.90560\n","Epoch 49/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0983 - acc: 0.9654 - val_loss: 0.4135 - val_acc: 0.9034\n","\n","Epoch 00049: val_acc did not improve from 0.90560\n","Epoch 50/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0931 - acc: 0.9676 - val_loss: 0.4278 - val_acc: 0.8942\n","\n","Epoch 00050: val_acc did not improve from 0.90560\n","Epoch 51/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0943 - acc: 0.9671 - val_loss: 0.3762 - val_acc: 0.8994\n","\n","Epoch 00051: val_acc did not improve from 0.90560\n","Epoch 52/100\n","704/704 [==============================] - 59s 84ms/step - loss: 0.0865 - acc: 0.9688 - val_loss: 0.3461 - val_acc: 0.9098\n","\n","Epoch 00052: val_acc improved from 0.90560 to 0.90980, saving model to best_model.h5\n","Epoch 53/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0904 - acc: 0.9689 - val_loss: 0.4303 - val_acc: 0.9002\n","\n","Epoch 00053: val_acc did not improve from 0.90980\n","Epoch 54/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0849 - acc: 0.9700 - val_loss: 0.4516 - val_acc: 0.8986\n","\n","Epoch 00054: val_acc did not improve from 0.90980\n","Epoch 55/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0855 - acc: 0.9707 - val_loss: 0.3803 - val_acc: 0.9080\n","\n","Epoch 00055: val_acc did not improve from 0.90980\n","Epoch 56/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0849 - acc: 0.9709 - val_loss: 0.3635 - val_acc: 0.9068\n","\n","Epoch 00056: val_acc did not improve from 0.90980\n","Epoch 57/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0838 - acc: 0.9699 - val_loss: 0.4066 - val_acc: 0.8994\n","\n","Epoch 00057: val_acc did not improve from 0.90980\n","Epoch 58/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0774 - acc: 0.9732 - val_loss: 0.3779 - val_acc: 0.9072\n","\n","Epoch 00058: val_acc did not improve from 0.90980\n","Epoch 59/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0783 - acc: 0.9722 - val_loss: 0.4644 - val_acc: 0.8962\n","\n","Epoch 00059: val_acc did not improve from 0.90980\n","Epoch 60/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0743 - acc: 0.9741 - val_loss: 0.3691 - val_acc: 0.9120\n","\n","Epoch 00060: val_acc improved from 0.90980 to 0.91200, saving model to best_model.h5\n","Epoch 61/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0761 - acc: 0.9732 - val_loss: 0.4118 - val_acc: 0.9016\n","\n","Epoch 00061: val_acc did not improve from 0.91200\n","Epoch 62/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0721 - acc: 0.9749 - val_loss: 0.3606 - val_acc: 0.9136\n","\n","Epoch 00062: val_acc improved from 0.91200 to 0.91360, saving model to best_model.h5\n","Epoch 63/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0766 - acc: 0.9730 - val_loss: 0.4502 - val_acc: 0.8956\n","\n","Epoch 00063: val_acc did not improve from 0.91360\n","Epoch 64/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0727 - acc: 0.9742 - val_loss: 0.4535 - val_acc: 0.9026\n","\n","Epoch 00064: val_acc did not improve from 0.91360\n","Epoch 65/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0695 - acc: 0.9759 - val_loss: 0.3780 - val_acc: 0.9056\n","\n","Epoch 00065: val_acc did not improve from 0.91360\n","Epoch 66/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0668 - acc: 0.9776 - val_loss: 0.4761 - val_acc: 0.8950\n","\n","Epoch 00066: val_acc did not improve from 0.91360\n","Epoch 67/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0666 - acc: 0.9769 - val_loss: 0.4245 - val_acc: 0.9042\n","\n","Epoch 00067: val_acc did not improve from 0.91360\n","Epoch 68/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0682 - acc: 0.9763 - val_loss: 0.4453 - val_acc: 0.9020\n","\n","Epoch 00068: val_acc did not improve from 0.91360\n","Epoch 69/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0662 - acc: 0.9766 - val_loss: 0.4348 - val_acc: 0.9036\n","\n","Epoch 00069: val_acc did not improve from 0.91360\n","Epoch 70/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0658 - acc: 0.9775 - val_loss: 0.4823 - val_acc: 0.8986\n","\n","Epoch 00070: val_acc did not improve from 0.91360\n","Epoch 71/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0689 - acc: 0.9767 - val_loss: 0.4088 - val_acc: 0.9044\n","\n","Epoch 00071: val_acc did not improve from 0.91360\n","Epoch 72/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0607 - acc: 0.9791 - val_loss: 0.3786 - val_acc: 0.9112\n","\n","Epoch 00072: val_acc did not improve from 0.91360\n","Epoch 73/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0617 - acc: 0.9796 - val_loss: 0.4476 - val_acc: 0.9046\n","\n","Epoch 00073: val_acc did not improve from 0.91360\n","Epoch 74/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0583 - acc: 0.9797 - val_loss: 0.4437 - val_acc: 0.9042\n","\n","Epoch 00074: val_acc did not improve from 0.91360\n","Epoch 75/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0609 - acc: 0.9781 - val_loss: 0.3754 - val_acc: 0.9138\n","\n","Epoch 00075: val_acc improved from 0.91360 to 0.91380, saving model to best_model.h5\n","Epoch 76/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0574 - acc: 0.9797 - val_loss: 0.4363 - val_acc: 0.9074\n","\n","Epoch 00076: val_acc did not improve from 0.91380\n","Epoch 77/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0549 - acc: 0.9810 - val_loss: 0.4718 - val_acc: 0.9042\n","\n","Epoch 00077: val_acc did not improve from 0.91380\n","Epoch 78/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0570 - acc: 0.9802 - val_loss: 0.4958 - val_acc: 0.8980\n","\n","Epoch 00078: val_acc did not improve from 0.91380\n","Epoch 79/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0550 - acc: 0.9813 - val_loss: 0.4503 - val_acc: 0.9012\n","\n","Epoch 00079: val_acc did not improve from 0.91380\n","Epoch 80/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0562 - acc: 0.9812 - val_loss: 0.4104 - val_acc: 0.9102\n","\n","Epoch 00080: val_acc did not improve from 0.91380\n","Epoch 81/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0544 - acc: 0.9817 - val_loss: 0.3970 - val_acc: 0.9130\n","\n","Epoch 00081: val_acc did not improve from 0.91380\n","Epoch 82/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0542 - acc: 0.9812 - val_loss: 0.4713 - val_acc: 0.9028\n","\n","Epoch 00082: val_acc did not improve from 0.91380\n","Epoch 83/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0535 - acc: 0.9815 - val_loss: 0.3816 - val_acc: 0.9138\n","\n","Epoch 00083: val_acc did not improve from 0.91380\n","Epoch 84/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0514 - acc: 0.9824 - val_loss: 0.4727 - val_acc: 0.9014\n","\n","Epoch 00084: val_acc did not improve from 0.91380\n","Epoch 85/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0507 - acc: 0.9828 - val_loss: 0.5768 - val_acc: 0.8950\n","\n","Epoch 00085: val_acc did not improve from 0.91380\n","Epoch 86/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0468 - acc: 0.9836 - val_loss: 0.4671 - val_acc: 0.9068\n","\n","Epoch 00086: val_acc did not improve from 0.91380\n","Epoch 87/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0498 - acc: 0.9835 - val_loss: 0.4221 - val_acc: 0.9064\n","\n","Epoch 00087: val_acc did not improve from 0.91380\n","Epoch 88/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0512 - acc: 0.9819 - val_loss: 0.4220 - val_acc: 0.9072\n","\n","Epoch 00088: val_acc did not improve from 0.91380\n","Epoch 89/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0470 - acc: 0.9840 - val_loss: 0.4030 - val_acc: 0.9118\n","\n","Epoch 00089: val_acc did not improve from 0.91380\n","Epoch 90/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0486 - acc: 0.9830 - val_loss: 0.4901 - val_acc: 0.9066\n","\n","Epoch 00090: val_acc did not improve from 0.91380\n","Epoch 91/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0446 - acc: 0.9836 - val_loss: 0.4285 - val_acc: 0.9112\n","\n","Epoch 00091: val_acc did not improve from 0.91380\n","Epoch 92/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0487 - acc: 0.9834 - val_loss: 0.4679 - val_acc: 0.9070\n","\n","Epoch 00092: val_acc did not improve from 0.91380\n","Epoch 93/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0444 - acc: 0.9849 - val_loss: 0.3920 - val_acc: 0.9156\n","\n","Epoch 00093: val_acc improved from 0.91380 to 0.91560, saving model to best_model.h5\n","Epoch 94/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0449 - acc: 0.9839 - val_loss: 0.3764 - val_acc: 0.9178\n","\n","Epoch 00094: val_acc improved from 0.91560 to 0.91780, saving model to best_model.h5\n","Epoch 95/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0440 - acc: 0.9853 - val_loss: 0.4006 - val_acc: 0.9160\n","\n","Epoch 00095: val_acc did not improve from 0.91780\n","Epoch 96/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0435 - acc: 0.9855 - val_loss: 0.3835 - val_acc: 0.9114\n","\n","Epoch 00096: val_acc did not improve from 0.91780\n","Epoch 97/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0425 - acc: 0.9849 - val_loss: 0.4466 - val_acc: 0.9114\n","\n","Epoch 00097: val_acc did not improve from 0.91780\n","Epoch 98/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0463 - acc: 0.9846 - val_loss: 0.3713 - val_acc: 0.9202\n","\n","Epoch 00098: val_acc improved from 0.91780 to 0.92020, saving model to best_model.h5\n","Epoch 99/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0431 - acc: 0.9855 - val_loss: 0.3944 - val_acc: 0.9174\n","\n","Epoch 00099: val_acc did not improve from 0.92020\n","Epoch 100/100\n","704/704 [==============================] - 60s 85ms/step - loss: 0.0446 - acc: 0.9857 - val_loss: 0.4791 - val_acc: 0.9006\n","\n","Epoch 00100: val_acc did not improve from 0.92020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BPs0sW1d2YVG","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"U6Smz5e8RT4m","colab_type":"code","outputId":"8eb93e7b-bf9b-44df-b8dc-eca012d8fd42","executionInfo":{"status":"ok","timestamp":1569598320044,"user_tz":-420,"elapsed":14519,"user":{"displayName":"Nhat trinh minh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeQT6iJxnWEHCPvudElEDL8bH4i6qDNE0jGGMkQA=s64","userId":"04755155104375278498"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["saved_model = load_model('best_model.h5')\n","# evaluate the model\n","\n","scores = saved_model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 5s 478us/step\n","Test loss: 0.414042059712857\n","Test accuracy: 0.9105\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qL0WFKqPgQxI","colab_type":"code","outputId":"88c8a986-120f-4400-92e3-1b018605fe5f","executionInfo":{"status":"ok","timestamp":1569591684357,"user_tz":-420,"elapsed":40854,"user":{"displayName":"Nhat trinh minh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeQT6iJxnWEHCPvudElEDL8bH4i6qDNE0jGGMkQA=s64","userId":"04755155104375278498"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z9m_ObiDgb_1","colab_type":"code","outputId":"09a8719f-8fad-48a6-a588-68e3c4e7b764","executionInfo":{"status":"ok","timestamp":1569598589985,"user_tz":-420,"elapsed":1578,"user":{"displayName":"Nhat trinh minh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeQT6iJxnWEHCPvudElEDL8bH4i6qDNE0jGGMkQA=s64","userId":"04755155104375278498"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%cd '/content/drive/My Drive/aominebka99'"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/aominebka99\n"],"name":"stdout"}]}]}