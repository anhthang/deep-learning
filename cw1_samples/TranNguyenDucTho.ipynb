{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_v4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJozlbCuWkFt",
        "colab_type": "code",
        "outputId": "25ca5740-fc19-4043-b8e9-64d024f7e693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obEHefJXVtDZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htZ1YKAXWnAh",
        "colab_type": "code",
        "outputId": "1957130e-2b40-44a3-c4c1-6a573fe4931e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, BatchNormalization, AveragePooling2D\n",
        "from keras import regularizers\n",
        "import os\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x, y), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x[0:40000]\n",
        "y_train = y[0:40000]\n",
        "\n",
        "x_val = x[40000:50000]\n",
        "y_val = y[40000:50000]\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_val.shape[0], 'validation samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (40000, 32, 32, 3)\n",
            "40000 train samples\n",
            "10000 validation samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7UD0FS3WnFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "  #define name stage\n",
        "  conv_name_base = 'conv' + str(stage)  + block + '_branch'\n",
        "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "  F1, F2, F3 = filters\n",
        "  X_shortcut = X\n",
        "  \n",
        "  # stage a\n",
        "  X = Conv2D(F1, (1,1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  \n",
        "  # stage b\n",
        "  \n",
        "  X = Conv2D(F2, (f,f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  \n",
        "  # stage c\n",
        "  \n",
        "  X = Conv2D(F3, (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "  \n",
        "  X = Add()([X_shortcut, X])\n",
        "  X = Activation('relu')(X)\n",
        "             \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfDCR793WnIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "  #define name stage \n",
        "  conv_name_base = 'conv' + str(stage) + block + 'branch'\n",
        "  bn_name_base = 'bn' +  str(stage) + block + '_branch'\n",
        "  F1, F2, F3 = filters\n",
        "  \n",
        "  X_shortcut = X\n",
        "  \n",
        "  \n",
        "  \n",
        "  # stage a\n",
        "  \n",
        "  X = Conv2D(F1, (1, 1), strides = (s, s), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  \n",
        "  # stage b\n",
        "  X = Conv2D(F2, (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  \n",
        "  # stage c\n",
        "  X = Conv2D(F3, (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "   \n",
        "  \n",
        "  \n",
        "  X_shortcut = Conv2D(F3, (1, 1), strides = (s, s), padding = 'valid', name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "  X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "  \n",
        "  X = Add()([X_shortcut, X])\n",
        "  X = Activation('relu')(X)\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rqURIZDwMTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StopTraining(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs = {}):\n",
        "    if(logs.get('val_acc') >  0.85):\n",
        "      print(\"\\nReached 85% on validation set!!! So stop training !!!\\n\")\n",
        "      self.model.stop_training = True;\n",
        "      \n",
        "      \n",
        "      \n",
        "StopTraining = StopTraining()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MoiH6qVwQ4t",
        "colab_type": "code",
        "outputId": "f516e403-c85f-434b-9cb3-4faaab22c14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_val /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp3_XXNZWnLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def model5(input_shape, classes):\n",
        "  X_input = Input(input_shape)\n",
        "  X = ZeroPadding2D((3, 3))(X_input)\n",
        "  \n",
        "  \n",
        "  # stage 1\n",
        "  \n",
        "  X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(0.1)(X)\n",
        "  X = MaxPooling2D((3, 3), strides = (2, 2))(X)\n",
        "  \n",
        "  # stage 2\n",
        "  X = convolutional_block(X, f = 3, filters = [32, 32, 128], stage = 2, block = 'a', s = 1)\n",
        "  X = Dropout(0.2)(X)\n",
        "  X = identity_block(X, 3, [32, 32, 128], stage = 2, block = 'b')\n",
        "  X = Dropout(0.2)(X)\n",
        "\n",
        "  # stage 3\n",
        "  X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 3, block = 'a', s = 1)\n",
        "  X = Dropout(0.3)(X)\n",
        "  X = identity_block(X, 3, [64, 64, 256], stage = 3, block = 'b')\n",
        "  X = Dropout(0.3)(X)\n",
        "  X = identity_block(X, 3, [64, 64, 256], stage = 3, block = 'c')\n",
        "  X = Dropout(0.3)(X)\n",
        "\n",
        "  # stage 4\n",
        "  X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 4, block = 'a', s = 1)\n",
        "  X = Dropout(0.4)(X)\n",
        "  \n",
        "  \n",
        "  X = AveragePooling2D((2, 2))(X)\n",
        "  \n",
        "  X = Flatten()(X)\n",
        "  X = Dense(256, activation = 'relu', name = 'fc' + str(256),kernel_regularizer=regularizers.l2(0.01), kernel_initializer = glorot_uniform(seed = 0))(X)\n",
        "  X = Dropout(0.4)(X)\n",
        "  \n",
        "  X = Dense(classes, activation = 'softmax', name = 'fc' + str(classes),kernel_regularizer=regularizers.l2(0.01), kernel_initializer = glorot_uniform(seed = 0))(X)\n",
        "  \n",
        "  \n",
        "  # Create Model \n",
        "  \n",
        "  model = Model(input = X_input, output = X, name = 'model3')\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V7LM2XxWnSs",
        "colab_type": "code",
        "outputId": "2232244b-c6a7-47aa-dd91-3242b145a50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model5 = model5(input_shape = (32, 32, 3), classes = 10)\n",
        "model5.summary()\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=0.009, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy',\n",
        "               optimizer = opt,\n",
        "               metrics = ['acc'])\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "\n",
        "model5.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=64),\n",
        "                        epochs=300,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        callbacks = [StopTraining],\n",
        "                        workers=4,\n",
        "                        verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"model3\", inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 38, 38, 3)    0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 16, 16, 64)   9472        zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv1 (BatchNormalization)   (None, 16, 16, 64)   256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 16, 16, 64)   0           bn_conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 16, 16, 64)   0           activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 64)     0           dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2abranch2a (Conv2D)         (None, 7, 7, 32)     2080        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2a (BatchNormalizati (None, 7, 7, 32)     128         conv2abranch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 7, 7, 32)     0           bn2a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2abranch2b (Conv2D)         (None, 7, 7, 32)     9248        activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2b (BatchNormalizati (None, 7, 7, 32)     128         conv2abranch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 7, 7, 32)     0           bn2a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2abranch1 (Conv2D)          (None, 7, 7, 128)    8320        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2abranch2c (Conv2D)         (None, 7, 7, 128)    4224        activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch1 (BatchNormalizatio (None, 7, 7, 128)    512         conv2abranch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2a_branch2c (BatchNormalizati (None, 7, 7, 128)    512         conv2abranch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 7, 7, 128)    0           bn2a_branch1[0][0]               \n",
            "                                                                 bn2a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 7, 7, 128)    0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 7, 7, 128)    0           activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2b_branch2a (Conv2D)        (None, 7, 7, 32)     4128        dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2a (BatchNormalizati (None, 7, 7, 32)     128         conv2b_branch2a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 7, 7, 32)     0           bn2b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2b_branch2b (Conv2D)        (None, 7, 7, 32)     9248        activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2b (BatchNormalizati (None, 7, 7, 32)     128         conv2b_branch2b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 7, 7, 32)     0           bn2b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2b_branch2c (Conv2D)        (None, 7, 7, 128)    4224        activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch2c (BatchNormalizati (None, 7, 7, 128)    512         conv2b_branch2c[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 7, 7, 128)    0           dropout_34[0][0]                 \n",
            "                                                                 bn2b_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 7, 7, 128)    0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 7, 7, 128)    0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3abranch2a (Conv2D)         (None, 7, 7, 64)     8256        dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2a (BatchNormalizati (None, 7, 7, 64)     256         conv3abranch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 7, 7, 64)     0           bn3a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3abranch2b (Conv2D)         (None, 7, 7, 64)     36928       activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2b (BatchNormalizati (None, 7, 7, 64)     256         conv3abranch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 7, 7, 64)     0           bn3a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3abranch1 (Conv2D)          (None, 7, 7, 256)    33024       dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3abranch2c (Conv2D)         (None, 7, 7, 256)    16640       activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch1 (BatchNormalizatio (None, 7, 7, 256)    1024        conv3abranch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3a_branch2c (BatchNormalizati (None, 7, 7, 256)    1024        conv3abranch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 7, 7, 256)    0           bn3a_branch1[0][0]               \n",
            "                                                                 bn3a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 7, 7, 256)    0           add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 7, 7, 256)    0           activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3b_branch2a (Conv2D)        (None, 7, 7, 64)     16448       dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2a (BatchNormalizati (None, 7, 7, 64)     256         conv3b_branch2a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 7, 7, 64)     0           bn3b_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3b_branch2b (Conv2D)        (None, 7, 7, 64)     36928       activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2b (BatchNormalizati (None, 7, 7, 64)     256         conv3b_branch2b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 7, 7, 64)     0           bn3b_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3b_branch2c (Conv2D)        (None, 7, 7, 256)    16640       activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch2c (BatchNormalizati (None, 7, 7, 256)    1024        conv3b_branch2c[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 7, 7, 256)    0           dropout_36[0][0]                 \n",
            "                                                                 bn3b_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 7, 7, 256)    0           add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 7, 7, 256)    0           activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3c_branch2a (Conv2D)        (None, 7, 7, 64)     16448       dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2a (BatchNormalizati (None, 7, 7, 64)     256         conv3c_branch2a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 7, 7, 64)     0           bn3c_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3c_branch2b (Conv2D)        (None, 7, 7, 64)     36928       activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2b (BatchNormalizati (None, 7, 7, 64)     256         conv3c_branch2b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 7, 7, 64)     0           bn3c_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3c_branch2c (Conv2D)        (None, 7, 7, 256)    16640       activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch2c (BatchNormalizati (None, 7, 7, 256)    1024        conv3c_branch2c[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 7, 7, 256)    0           dropout_37[0][0]                 \n",
            "                                                                 bn3c_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 7, 7, 256)    0           add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 7, 7, 256)    0           activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4abranch2a (Conv2D)         (None, 7, 7, 128)    32896       dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2a (BatchNormalizati (None, 7, 7, 128)    512         conv4abranch2a[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 7, 7, 128)    0           bn4a_branch2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4abranch2b (Conv2D)         (None, 7, 7, 128)    147584      activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2b (BatchNormalizati (None, 7, 7, 128)    512         conv4abranch2b[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 7, 7, 128)    0           bn4a_branch2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4abranch1 (Conv2D)          (None, 7, 7, 512)    131584      dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4abranch2c (Conv2D)         (None, 7, 7, 512)    66048       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch1 (BatchNormalizatio (None, 7, 7, 512)    2048        conv4abranch1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bn4a_branch2c (BatchNormalizati (None, 7, 7, 512)    2048        conv4abranch2c[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 7, 7, 512)    0           bn4a_branch1[0][0]               \n",
            "                                                                 bn4a_branch2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 7, 7, 512)    0           add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 7, 7, 512)    0           activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 3, 3, 512)    0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 4608)         0           average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "fc256 (Dense)                   (None, 256)          1179904     flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 256)          0           fc256[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "fc10 (Dense)                    (None, 10)           2570        dropout_40[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,859,466\n",
            "Trainable params: 1,852,938\n",
            "Non-trainable params: 6,528\n",
            "__________________________________________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/300\n",
            " - 56s - loss: 2.9033 - acc: 0.2096 - val_loss: 2.4611 - val_acc: 0.1871\n",
            "Epoch 2/300\n",
            " - 45s - loss: 1.8770 - acc: 0.3203 - val_loss: 1.7577 - val_acc: 0.3647\n",
            "Epoch 3/300\n",
            " - 45s - loss: 1.7876 - acc: 0.3597 - val_loss: 1.9698 - val_acc: 0.2775\n",
            "Epoch 4/300\n",
            " - 45s - loss: 1.7301 - acc: 0.3903 - val_loss: 2.0153 - val_acc: 0.3352\n",
            "Epoch 5/300\n",
            " - 45s - loss: 1.6796 - acc: 0.4151 - val_loss: 2.2056 - val_acc: 0.3011\n",
            "Epoch 6/300\n",
            " - 45s - loss: 1.6303 - acc: 0.4440 - val_loss: 1.7468 - val_acc: 0.4469\n",
            "Epoch 7/300\n",
            " - 45s - loss: 1.5881 - acc: 0.4638 - val_loss: 1.4690 - val_acc: 0.5133\n",
            "Epoch 8/300\n",
            " - 45s - loss: 1.5411 - acc: 0.4890 - val_loss: 1.3966 - val_acc: 0.5544\n",
            "Epoch 9/300\n",
            " - 45s - loss: 1.4991 - acc: 0.5160 - val_loss: 1.5080 - val_acc: 0.5044\n",
            "Epoch 10/300\n",
            " - 45s - loss: 1.4649 - acc: 0.5319 - val_loss: 1.5386 - val_acc: 0.5361\n",
            "Epoch 11/300\n",
            " - 45s - loss: 1.4222 - acc: 0.5480 - val_loss: 1.3981 - val_acc: 0.5578\n",
            "Epoch 12/300\n",
            " - 45s - loss: 1.3907 - acc: 0.5636 - val_loss: 1.2549 - val_acc: 0.6092\n",
            "Epoch 13/300\n",
            " - 45s - loss: 1.3646 - acc: 0.5733 - val_loss: 1.3566 - val_acc: 0.5729\n",
            "Epoch 14/300\n",
            " - 45s - loss: 1.3323 - acc: 0.5846 - val_loss: 1.3354 - val_acc: 0.5847\n",
            "Epoch 15/300\n",
            " - 45s - loss: 1.3228 - acc: 0.5882 - val_loss: 1.4946 - val_acc: 0.5641\n",
            "Epoch 16/300\n",
            " - 45s - loss: 1.3015 - acc: 0.5993 - val_loss: 1.4225 - val_acc: 0.5776\n",
            "Epoch 17/300\n",
            " - 45s - loss: 1.2697 - acc: 0.6109 - val_loss: 1.3564 - val_acc: 0.5950\n",
            "Epoch 18/300\n",
            " - 45s - loss: 1.2518 - acc: 0.6145 - val_loss: 1.4413 - val_acc: 0.5959\n",
            "Epoch 19/300\n",
            " - 45s - loss: 1.2504 - acc: 0.6191 - val_loss: 1.2499 - val_acc: 0.6248\n",
            "Epoch 20/300\n",
            " - 45s - loss: 1.2300 - acc: 0.6314 - val_loss: 1.4066 - val_acc: 0.5923\n",
            "Epoch 21/300\n",
            " - 45s - loss: 1.2160 - acc: 0.6345 - val_loss: 1.2096 - val_acc: 0.6336\n",
            "Epoch 22/300\n",
            " - 45s - loss: 1.2020 - acc: 0.6367 - val_loss: 1.1224 - val_acc: 0.6611\n",
            "Epoch 23/300\n",
            " - 45s - loss: 1.1925 - acc: 0.6438 - val_loss: 1.3357 - val_acc: 0.6327\n",
            "Epoch 24/300\n",
            " - 45s - loss: 1.1842 - acc: 0.6485 - val_loss: 1.3658 - val_acc: 0.6040\n",
            "Epoch 25/300\n",
            " - 45s - loss: 1.1734 - acc: 0.6517 - val_loss: 1.1441 - val_acc: 0.6624\n",
            "Epoch 26/300\n",
            " - 45s - loss: 1.1489 - acc: 0.6609 - val_loss: 1.2702 - val_acc: 0.6387\n",
            "Epoch 27/300\n",
            " - 45s - loss: 1.1374 - acc: 0.6632 - val_loss: 1.3858 - val_acc: 0.6381\n",
            "Epoch 28/300\n",
            " - 45s - loss: 1.1440 - acc: 0.6643 - val_loss: 1.2985 - val_acc: 0.6153\n",
            "Epoch 29/300\n",
            " - 45s - loss: 1.1274 - acc: 0.6698 - val_loss: 1.2366 - val_acc: 0.6345\n",
            "Epoch 30/300\n",
            " - 45s - loss: 1.1294 - acc: 0.6734 - val_loss: 1.3397 - val_acc: 0.6305\n",
            "Epoch 31/300\n",
            " - 45s - loss: 1.1207 - acc: 0.6725 - val_loss: 1.2890 - val_acc: 0.6474\n",
            "Epoch 32/300\n",
            " - 45s - loss: 1.0983 - acc: 0.6782 - val_loss: 1.0698 - val_acc: 0.6934\n",
            "Epoch 33/300\n",
            " - 45s - loss: 1.0898 - acc: 0.6816 - val_loss: 1.1737 - val_acc: 0.6762\n",
            "Epoch 34/300\n",
            " - 45s - loss: 1.0920 - acc: 0.6859 - val_loss: 1.0433 - val_acc: 0.6909\n",
            "Epoch 35/300\n",
            " - 45s - loss: 1.0869 - acc: 0.6859 - val_loss: 1.0867 - val_acc: 0.7006\n",
            "Epoch 36/300\n",
            " - 45s - loss: 1.0738 - acc: 0.6920 - val_loss: 0.9774 - val_acc: 0.7095\n",
            "Epoch 37/300\n",
            " - 45s - loss: 1.0747 - acc: 0.6946 - val_loss: 1.0945 - val_acc: 0.6914\n",
            "Epoch 38/300\n",
            " - 45s - loss: 1.0623 - acc: 0.6957 - val_loss: 1.3745 - val_acc: 0.6391\n",
            "Epoch 39/300\n",
            " - 45s - loss: 1.0596 - acc: 0.6966 - val_loss: 1.1723 - val_acc: 0.6768\n",
            "Epoch 40/300\n",
            " - 45s - loss: 1.0440 - acc: 0.7021 - val_loss: 0.9952 - val_acc: 0.7269\n",
            "Epoch 41/300\n",
            " - 45s - loss: 1.0428 - acc: 0.7038 - val_loss: 2.1269 - val_acc: 0.5745\n",
            "Epoch 42/300\n",
            " - 45s - loss: 1.0342 - acc: 0.7042 - val_loss: 1.4368 - val_acc: 0.6668\n",
            "Epoch 43/300\n",
            " - 45s - loss: 1.0250 - acc: 0.7110 - val_loss: 1.0206 - val_acc: 0.7160\n",
            "Epoch 44/300\n",
            " - 45s - loss: 1.0199 - acc: 0.7124 - val_loss: 1.2994 - val_acc: 0.6521\n",
            "Epoch 45/300\n",
            " - 45s - loss: 1.0192 - acc: 0.7147 - val_loss: 1.0048 - val_acc: 0.7150\n",
            "Epoch 46/300\n",
            " - 45s - loss: 1.0109 - acc: 0.7187 - val_loss: 1.0525 - val_acc: 0.7009\n",
            "Epoch 47/300\n",
            " - 45s - loss: 1.0121 - acc: 0.7190 - val_loss: 1.1588 - val_acc: 0.6800\n",
            "Epoch 48/300\n",
            " - 45s - loss: 1.0049 - acc: 0.7197 - val_loss: 1.5727 - val_acc: 0.6322\n",
            "Epoch 49/300\n",
            " - 45s - loss: 1.0059 - acc: 0.7196 - val_loss: 1.2391 - val_acc: 0.6926\n",
            "Epoch 50/300\n",
            " - 45s - loss: 0.9956 - acc: 0.7202 - val_loss: 0.9365 - val_acc: 0.7317\n",
            "Epoch 51/300\n",
            " - 45s - loss: 0.9986 - acc: 0.7208 - val_loss: 1.1186 - val_acc: 0.7077\n",
            "Epoch 52/300\n",
            " - 45s - loss: 0.9920 - acc: 0.7256 - val_loss: 1.3994 - val_acc: 0.6581\n",
            "Epoch 53/300\n",
            " - 45s - loss: 0.9813 - acc: 0.7306 - val_loss: 1.0391 - val_acc: 0.7152\n",
            "Epoch 54/300\n",
            " - 45s - loss: 0.9741 - acc: 0.7290 - val_loss: 0.9184 - val_acc: 0.7551\n",
            "Epoch 55/300\n",
            " - 45s - loss: 0.9792 - acc: 0.7323 - val_loss: 0.9019 - val_acc: 0.7491\n",
            "Epoch 56/300\n",
            " - 45s - loss: 0.9688 - acc: 0.7311 - val_loss: 0.8885 - val_acc: 0.7606\n",
            "Epoch 57/300\n",
            " - 45s - loss: 0.9717 - acc: 0.7340 - val_loss: 1.2282 - val_acc: 0.7091\n",
            "Epoch 58/300\n",
            " - 45s - loss: 0.9545 - acc: 0.7364 - val_loss: 0.8991 - val_acc: 0.7501\n",
            "Epoch 59/300\n",
            " - 45s - loss: 0.9534 - acc: 0.7410 - val_loss: 0.9721 - val_acc: 0.7410\n",
            "Epoch 60/300\n",
            " - 45s - loss: 0.9479 - acc: 0.7399 - val_loss: 1.1417 - val_acc: 0.7145\n",
            "Epoch 61/300\n",
            " - 45s - loss: 0.9524 - acc: 0.7422 - val_loss: 0.9954 - val_acc: 0.7433\n",
            "Epoch 62/300\n",
            " - 45s - loss: 0.9410 - acc: 0.7436 - val_loss: 0.9875 - val_acc: 0.7434\n",
            "Epoch 63/300\n",
            " - 45s - loss: 0.9382 - acc: 0.7434 - val_loss: 1.0634 - val_acc: 0.7141\n",
            "Epoch 64/300\n",
            " - 45s - loss: 0.9398 - acc: 0.7422 - val_loss: 1.1947 - val_acc: 0.6959\n",
            "Epoch 65/300\n",
            " - 45s - loss: 0.9345 - acc: 0.7429 - val_loss: 0.8655 - val_acc: 0.7651\n",
            "Epoch 66/300\n",
            " - 45s - loss: 0.9244 - acc: 0.7498 - val_loss: 0.9782 - val_acc: 0.7389\n",
            "Epoch 67/300\n",
            " - 45s - loss: 0.9275 - acc: 0.7483 - val_loss: 1.5803 - val_acc: 0.6667\n",
            "Epoch 68/300\n",
            " - 44s - loss: 0.9248 - acc: 0.7510 - val_loss: 0.9528 - val_acc: 0.7564\n",
            "Epoch 69/300\n",
            " - 44s - loss: 0.9112 - acc: 0.7540 - val_loss: 1.0303 - val_acc: 0.7354\n",
            "Epoch 70/300\n",
            " - 45s - loss: 0.9265 - acc: 0.7498 - val_loss: 1.0263 - val_acc: 0.7454\n",
            "Epoch 71/300\n",
            " - 45s - loss: 0.9194 - acc: 0.7521 - val_loss: 0.9988 - val_acc: 0.7446\n",
            "Epoch 72/300\n",
            " - 44s - loss: 0.9236 - acc: 0.7496 - val_loss: 1.2266 - val_acc: 0.7087\n",
            "Epoch 73/300\n",
            " - 45s - loss: 0.9152 - acc: 0.7530 - val_loss: 1.0305 - val_acc: 0.7315\n",
            "Epoch 74/300\n",
            " - 45s - loss: 0.9042 - acc: 0.7545 - val_loss: 0.9556 - val_acc: 0.7665\n",
            "Epoch 75/300\n",
            " - 45s - loss: 0.9185 - acc: 0.7533 - val_loss: 0.9155 - val_acc: 0.7468\n",
            "Epoch 76/300\n",
            " - 45s - loss: 0.9055 - acc: 0.7551 - val_loss: 0.9465 - val_acc: 0.7527\n",
            "Epoch 77/300\n",
            " - 45s - loss: 0.9007 - acc: 0.7568 - val_loss: 0.9898 - val_acc: 0.7493\n",
            "Epoch 78/300\n",
            " - 45s - loss: 0.8974 - acc: 0.7584 - val_loss: 1.1799 - val_acc: 0.7019\n",
            "Epoch 79/300\n",
            " - 44s - loss: 0.8976 - acc: 0.7591 - val_loss: 0.9904 - val_acc: 0.7408\n",
            "Epoch 80/300\n",
            " - 45s - loss: 0.8926 - acc: 0.7608 - val_loss: 0.9688 - val_acc: 0.7602\n",
            "Epoch 81/300\n",
            " - 45s - loss: 0.8985 - acc: 0.7613 - val_loss: 0.9311 - val_acc: 0.7462\n",
            "Epoch 82/300\n",
            " - 45s - loss: 0.8862 - acc: 0.7639 - val_loss: 1.0513 - val_acc: 0.7256\n",
            "Epoch 83/300\n",
            " - 45s - loss: 0.8794 - acc: 0.7653 - val_loss: 1.0165 - val_acc: 0.7489\n",
            "Epoch 84/300\n",
            " - 45s - loss: 0.8849 - acc: 0.7649 - val_loss: 1.0360 - val_acc: 0.7358\n",
            "Epoch 85/300\n",
            " - 45s - loss: 0.8792 - acc: 0.7659 - val_loss: 0.8325 - val_acc: 0.7880\n",
            "Epoch 86/300\n",
            " - 45s - loss: 0.8706 - acc: 0.7699 - val_loss: 1.0652 - val_acc: 0.7327\n",
            "Epoch 87/300\n",
            " - 45s - loss: 0.8737 - acc: 0.7673 - val_loss: 0.9312 - val_acc: 0.7671\n",
            "Epoch 88/300\n",
            " - 45s - loss: 0.8781 - acc: 0.7697 - val_loss: 0.9312 - val_acc: 0.7552\n",
            "Epoch 89/300\n",
            " - 45s - loss: 0.8789 - acc: 0.7672 - val_loss: 1.0107 - val_acc: 0.7418\n",
            "Epoch 90/300\n",
            " - 45s - loss: 0.8589 - acc: 0.7703 - val_loss: 1.0378 - val_acc: 0.7360\n",
            "Epoch 91/300\n",
            " - 45s - loss: 0.8676 - acc: 0.7670 - val_loss: 0.9101 - val_acc: 0.7595\n",
            "Epoch 92/300\n",
            " - 45s - loss: 0.8602 - acc: 0.7710 - val_loss: 0.8400 - val_acc: 0.7793\n",
            "Epoch 93/300\n",
            " - 45s - loss: 0.8661 - acc: 0.7754 - val_loss: 0.9533 - val_acc: 0.7579\n",
            "Epoch 94/300\n",
            " - 45s - loss: 0.8546 - acc: 0.7726 - val_loss: 0.8673 - val_acc: 0.7767\n",
            "Epoch 95/300\n",
            " - 44s - loss: 0.8602 - acc: 0.7736 - val_loss: 0.8805 - val_acc: 0.7698\n",
            "Epoch 96/300\n",
            " - 44s - loss: 0.8562 - acc: 0.7774 - val_loss: 0.8496 - val_acc: 0.7821\n",
            "Epoch 97/300\n",
            " - 45s - loss: 0.8477 - acc: 0.7763 - val_loss: 0.8990 - val_acc: 0.7551\n",
            "Epoch 98/300\n",
            " - 44s - loss: 0.8542 - acc: 0.7738 - val_loss: 0.8830 - val_acc: 0.7856\n",
            "Epoch 99/300\n",
            " - 44s - loss: 0.8557 - acc: 0.7744 - val_loss: 0.9825 - val_acc: 0.7525\n",
            "Epoch 100/300\n",
            " - 44s - loss: 0.8584 - acc: 0.7760 - val_loss: 0.9584 - val_acc: 0.7575\n",
            "Epoch 101/300\n",
            " - 45s - loss: 0.8462 - acc: 0.7771 - val_loss: 0.9293 - val_acc: 0.7671\n",
            "Epoch 102/300\n",
            " - 44s - loss: 0.8420 - acc: 0.7776 - val_loss: 0.8428 - val_acc: 0.7847\n",
            "Epoch 103/300\n",
            " - 44s - loss: 0.8332 - acc: 0.7806 - val_loss: 1.1368 - val_acc: 0.7380\n",
            "Epoch 104/300\n",
            " - 44s - loss: 0.8291 - acc: 0.7810 - val_loss: 0.9437 - val_acc: 0.7656\n",
            "Epoch 105/300\n",
            " - 44s - loss: 0.8393 - acc: 0.7799 - val_loss: 0.9401 - val_acc: 0.7676\n",
            "Epoch 106/300\n",
            " - 44s - loss: 0.8405 - acc: 0.7797 - val_loss: 0.8504 - val_acc: 0.7879\n",
            "Epoch 107/300\n",
            " - 45s - loss: 0.8358 - acc: 0.7792 - val_loss: 0.9106 - val_acc: 0.7734\n",
            "Epoch 108/300\n",
            " - 44s - loss: 0.8290 - acc: 0.7822 - val_loss: 1.0887 - val_acc: 0.7401\n",
            "Epoch 109/300\n",
            " - 44s - loss: 0.8344 - acc: 0.7802 - val_loss: 0.8532 - val_acc: 0.7842\n",
            "Epoch 110/300\n",
            " - 44s - loss: 0.8197 - acc: 0.7863 - val_loss: 0.9694 - val_acc: 0.7741\n",
            "Epoch 111/300\n",
            " - 45s - loss: 0.8274 - acc: 0.7819 - val_loss: 0.9667 - val_acc: 0.7709\n",
            "Epoch 112/300\n",
            " - 45s - loss: 0.8236 - acc: 0.7847 - val_loss: 0.8723 - val_acc: 0.7901\n",
            "Epoch 113/300\n",
            " - 44s - loss: 0.8226 - acc: 0.7865 - val_loss: 0.8859 - val_acc: 0.7914\n",
            "Epoch 114/300\n",
            " - 44s - loss: 0.8163 - acc: 0.7872 - val_loss: 0.9255 - val_acc: 0.7653\n",
            "Epoch 115/300\n",
            " - 44s - loss: 0.8243 - acc: 0.7875 - val_loss: 0.8903 - val_acc: 0.7633\n",
            "Epoch 116/300\n",
            " - 45s - loss: 0.8167 - acc: 0.7878 - val_loss: 0.8335 - val_acc: 0.7878\n",
            "Epoch 117/300\n",
            " - 45s - loss: 0.8163 - acc: 0.7869 - val_loss: 1.2610 - val_acc: 0.7393\n",
            "Epoch 118/300\n",
            " - 45s - loss: 0.8195 - acc: 0.7861 - val_loss: 0.8286 - val_acc: 0.7905\n",
            "Epoch 119/300\n",
            " - 45s - loss: 0.8051 - acc: 0.7901 - val_loss: 0.9724 - val_acc: 0.7713\n",
            "Epoch 120/300\n",
            " - 45s - loss: 0.7971 - acc: 0.7944 - val_loss: 1.3078 - val_acc: 0.7473\n",
            "Epoch 121/300\n",
            " - 45s - loss: 0.8085 - acc: 0.7901 - val_loss: 0.8742 - val_acc: 0.7779\n",
            "Epoch 122/300\n",
            " - 45s - loss: 0.8044 - acc: 0.7918 - val_loss: 0.8680 - val_acc: 0.7895\n",
            "Epoch 123/300\n",
            " - 45s - loss: 0.8061 - acc: 0.7903 - val_loss: 0.9636 - val_acc: 0.7709\n",
            "Epoch 124/300\n",
            " - 45s - loss: 0.8079 - acc: 0.7928 - val_loss: 0.8894 - val_acc: 0.7758\n",
            "Epoch 125/300\n",
            " - 45s - loss: 0.8017 - acc: 0.7927 - val_loss: 0.9674 - val_acc: 0.7776\n",
            "Epoch 126/300\n",
            " - 45s - loss: 0.7949 - acc: 0.7922 - val_loss: 0.8947 - val_acc: 0.7822\n",
            "Epoch 127/300\n",
            " - 45s - loss: 0.7999 - acc: 0.7906 - val_loss: 0.8026 - val_acc: 0.7916\n",
            "Epoch 128/300\n",
            " - 45s - loss: 0.7966 - acc: 0.7920 - val_loss: 0.8075 - val_acc: 0.7976\n",
            "Epoch 129/300\n",
            " - 44s - loss: 0.7888 - acc: 0.7965 - val_loss: 0.8508 - val_acc: 0.7856\n",
            "Epoch 130/300\n",
            " - 44s - loss: 0.7878 - acc: 0.7957 - val_loss: 0.9606 - val_acc: 0.7657\n",
            "Epoch 131/300\n",
            " - 45s - loss: 0.7853 - acc: 0.7969 - val_loss: 0.9775 - val_acc: 0.7633\n",
            "Epoch 132/300\n",
            " - 44s - loss: 0.7894 - acc: 0.7962 - val_loss: 0.8988 - val_acc: 0.7863\n",
            "Epoch 133/300\n",
            " - 45s - loss: 0.7899 - acc: 0.7971 - val_loss: 0.8525 - val_acc: 0.7875\n",
            "Epoch 134/300\n",
            " - 44s - loss: 0.7983 - acc: 0.7939 - val_loss: 0.9068 - val_acc: 0.7809\n",
            "Epoch 135/300\n",
            " - 44s - loss: 0.7782 - acc: 0.7979 - val_loss: 0.8508 - val_acc: 0.7867\n",
            "Epoch 136/300\n",
            " - 45s - loss: 0.7829 - acc: 0.7950 - val_loss: 0.7772 - val_acc: 0.8118\n",
            "Epoch 137/300\n",
            " - 44s - loss: 0.7856 - acc: 0.7968 - val_loss: 1.1501 - val_acc: 0.7396\n",
            "Epoch 138/300\n",
            " - 44s - loss: 0.7837 - acc: 0.7963 - val_loss: 0.8388 - val_acc: 0.7816\n",
            "Epoch 139/300\n",
            " - 45s - loss: 0.7799 - acc: 0.7985 - val_loss: 0.9087 - val_acc: 0.7873\n",
            "Epoch 140/300\n",
            " - 45s - loss: 0.7815 - acc: 0.8033 - val_loss: 0.8186 - val_acc: 0.7964\n",
            "Epoch 141/300\n",
            " - 45s - loss: 0.7787 - acc: 0.8007 - val_loss: 0.7711 - val_acc: 0.8141\n",
            "Epoch 142/300\n",
            " - 45s - loss: 0.7758 - acc: 0.8024 - val_loss: 0.9406 - val_acc: 0.7791\n",
            "Epoch 143/300\n",
            " - 45s - loss: 0.7682 - acc: 0.8038 - val_loss: 0.8926 - val_acc: 0.7824\n",
            "Epoch 144/300\n",
            " - 44s - loss: 0.7725 - acc: 0.8018 - val_loss: 0.7602 - val_acc: 0.8110\n",
            "Epoch 145/300\n",
            " - 44s - loss: 0.7711 - acc: 0.8007 - val_loss: 0.8024 - val_acc: 0.8028\n",
            "Epoch 146/300\n",
            " - 45s - loss: 0.7648 - acc: 0.8033 - val_loss: 0.8112 - val_acc: 0.8069\n",
            "Epoch 147/300\n",
            " - 45s - loss: 0.7685 - acc: 0.8024 - val_loss: 0.8872 - val_acc: 0.7966\n",
            "Epoch 148/300\n",
            " - 45s - loss: 0.7707 - acc: 0.8020 - val_loss: 0.7856 - val_acc: 0.8063\n",
            "Epoch 149/300\n",
            " - 45s - loss: 0.7748 - acc: 0.7984 - val_loss: 0.8032 - val_acc: 0.7976\n",
            "Epoch 150/300\n",
            " - 45s - loss: 0.7625 - acc: 0.8048 - val_loss: 0.8976 - val_acc: 0.7791\n",
            "Epoch 151/300\n",
            " - 45s - loss: 0.7641 - acc: 0.8052 - val_loss: 0.8445 - val_acc: 0.7998\n",
            "Epoch 152/300\n",
            " - 45s - loss: 0.7638 - acc: 0.8034 - val_loss: 0.8660 - val_acc: 0.7851\n",
            "Epoch 153/300\n",
            " - 45s - loss: 0.7690 - acc: 0.8011 - val_loss: 0.7638 - val_acc: 0.8056\n",
            "Epoch 154/300\n",
            " - 45s - loss: 0.7492 - acc: 0.8083 - val_loss: 0.8211 - val_acc: 0.7882\n",
            "Epoch 155/300\n",
            " - 45s - loss: 0.7565 - acc: 0.8068 - val_loss: 0.8245 - val_acc: 0.8114\n",
            "Epoch 156/300\n",
            " - 45s - loss: 0.7507 - acc: 0.8077 - val_loss: 0.8402 - val_acc: 0.7974\n",
            "Epoch 157/300\n",
            " - 45s - loss: 0.7515 - acc: 0.8066 - val_loss: 0.8287 - val_acc: 0.7977\n",
            "Epoch 158/300\n",
            " - 45s - loss: 0.7580 - acc: 0.8059 - val_loss: 0.8745 - val_acc: 0.7888\n",
            "Epoch 159/300\n",
            " - 45s - loss: 0.7570 - acc: 0.8058 - val_loss: 0.9522 - val_acc: 0.7758\n",
            "Epoch 160/300\n",
            " - 45s - loss: 0.7551 - acc: 0.8074 - val_loss: 0.9344 - val_acc: 0.7897\n",
            "Epoch 161/300\n",
            " - 45s - loss: 0.7581 - acc: 0.8044 - val_loss: 0.8991 - val_acc: 0.7886\n",
            "Epoch 162/300\n",
            " - 45s - loss: 0.7432 - acc: 0.8106 - val_loss: 0.7840 - val_acc: 0.8046\n",
            "Epoch 163/300\n",
            " - 45s - loss: 0.7534 - acc: 0.8085 - val_loss: 0.9209 - val_acc: 0.7920\n",
            "Epoch 164/300\n",
            " - 45s - loss: 0.7445 - acc: 0.8112 - val_loss: 0.8155 - val_acc: 0.8018\n",
            "Epoch 165/300\n",
            " - 45s - loss: 0.7542 - acc: 0.8074 - val_loss: 0.8646 - val_acc: 0.7879\n",
            "Epoch 166/300\n",
            " - 45s - loss: 0.7494 - acc: 0.8081 - val_loss: 0.8254 - val_acc: 0.7922\n",
            "Epoch 167/300\n",
            " - 45s - loss: 0.7494 - acc: 0.8087 - val_loss: 0.9197 - val_acc: 0.7893\n",
            "Epoch 168/300\n",
            " - 45s - loss: 0.7420 - acc: 0.8093 - val_loss: 0.9329 - val_acc: 0.7860\n",
            "Epoch 169/300\n",
            " - 45s - loss: 0.7373 - acc: 0.8100 - val_loss: 0.9971 - val_acc: 0.7706\n",
            "Epoch 170/300\n",
            " - 45s - loss: 0.7432 - acc: 0.8102 - val_loss: 1.0119 - val_acc: 0.7626\n",
            "Epoch 171/300\n",
            " - 45s - loss: 0.7456 - acc: 0.8087 - val_loss: 0.8148 - val_acc: 0.8036\n",
            "Epoch 172/300\n",
            " - 45s - loss: 0.7343 - acc: 0.8097 - val_loss: 0.7731 - val_acc: 0.8027\n",
            "Epoch 173/300\n",
            " - 45s - loss: 0.7309 - acc: 0.8129 - val_loss: 0.8168 - val_acc: 0.8058\n",
            "Epoch 174/300\n",
            " - 45s - loss: 0.7352 - acc: 0.8139 - val_loss: 0.8509 - val_acc: 0.8106\n",
            "Epoch 175/300\n",
            " - 45s - loss: 0.7424 - acc: 0.8131 - val_loss: 0.8161 - val_acc: 0.8070\n",
            "Epoch 176/300\n",
            " - 45s - loss: 0.7264 - acc: 0.8150 - val_loss: 1.0117 - val_acc: 0.7670\n",
            "Epoch 177/300\n",
            " - 44s - loss: 0.7319 - acc: 0.8142 - val_loss: 0.7883 - val_acc: 0.8109\n",
            "Epoch 178/300\n",
            " - 44s - loss: 0.7318 - acc: 0.8141 - val_loss: 0.8661 - val_acc: 0.7907\n",
            "Epoch 179/300\n",
            " - 44s - loss: 0.7349 - acc: 0.8125 - val_loss: 1.0005 - val_acc: 0.7725\n",
            "Epoch 180/300\n",
            " - 44s - loss: 0.7286 - acc: 0.8133 - val_loss: 1.0133 - val_acc: 0.7660\n",
            "Epoch 181/300\n",
            " - 45s - loss: 0.7224 - acc: 0.8185 - val_loss: 0.9107 - val_acc: 0.7854\n",
            "Epoch 182/300\n",
            " - 44s - loss: 0.7327 - acc: 0.8156 - val_loss: 0.7737 - val_acc: 0.8189\n",
            "Epoch 183/300\n",
            " - 44s - loss: 0.7230 - acc: 0.8164 - val_loss: 0.7560 - val_acc: 0.8189\n",
            "Epoch 184/300\n",
            " - 45s - loss: 0.7324 - acc: 0.8149 - val_loss: 0.7624 - val_acc: 0.8234\n",
            "Epoch 185/300\n",
            " - 44s - loss: 0.7267 - acc: 0.8161 - val_loss: 0.8233 - val_acc: 0.8118\n",
            "Epoch 186/300\n",
            " - 44s - loss: 0.7118 - acc: 0.8182 - val_loss: 0.8879 - val_acc: 0.8037\n",
            "Epoch 187/300\n",
            " - 45s - loss: 0.7332 - acc: 0.8145 - val_loss: 1.0494 - val_acc: 0.7733\n",
            "Epoch 188/300\n",
            " - 44s - loss: 0.7260 - acc: 0.8155 - val_loss: 0.7954 - val_acc: 0.8096\n",
            "Epoch 189/300\n",
            " - 44s - loss: 0.7255 - acc: 0.8165 - val_loss: 0.7644 - val_acc: 0.8201\n",
            "Epoch 190/300\n",
            " - 44s - loss: 0.7211 - acc: 0.8156 - val_loss: 1.1064 - val_acc: 0.7643\n",
            "Epoch 191/300\n",
            " - 44s - loss: 0.7182 - acc: 0.8173 - val_loss: 0.8004 - val_acc: 0.8012\n",
            "Epoch 192/300\n",
            " - 45s - loss: 0.7236 - acc: 0.8166 - val_loss: 0.8278 - val_acc: 0.8100\n",
            "Epoch 193/300\n",
            " - 44s - loss: 0.7169 - acc: 0.8173 - val_loss: 0.7702 - val_acc: 0.8084\n",
            "Epoch 194/300\n",
            " - 44s - loss: 0.7224 - acc: 0.8190 - val_loss: 0.9110 - val_acc: 0.8113\n",
            "Epoch 195/300\n",
            " - 44s - loss: 0.7090 - acc: 0.8209 - val_loss: 0.8234 - val_acc: 0.8053\n",
            "Epoch 196/300\n",
            " - 44s - loss: 0.7183 - acc: 0.8173 - val_loss: 1.0937 - val_acc: 0.7535\n",
            "Epoch 197/300\n",
            " - 44s - loss: 0.7117 - acc: 0.8175 - val_loss: 0.9393 - val_acc: 0.7747\n",
            "Epoch 198/300\n",
            " - 44s - loss: 0.7204 - acc: 0.8184 - val_loss: 0.8174 - val_acc: 0.8125\n",
            "Epoch 199/300\n",
            " - 44s - loss: 0.7042 - acc: 0.8216 - val_loss: 0.8615 - val_acc: 0.8090\n",
            "Epoch 200/300\n",
            " - 45s - loss: 0.7140 - acc: 0.8224 - val_loss: 0.8150 - val_acc: 0.8164\n",
            "Epoch 201/300\n",
            " - 45s - loss: 0.7088 - acc: 0.8184 - val_loss: 0.7858 - val_acc: 0.8076\n",
            "Epoch 202/300\n",
            " - 45s - loss: 0.6983 - acc: 0.8238 - val_loss: 1.0987 - val_acc: 0.7636\n",
            "Epoch 203/300\n",
            " - 44s - loss: 0.7111 - acc: 0.8208 - val_loss: 0.8041 - val_acc: 0.8092\n",
            "Epoch 204/300\n",
            " - 45s - loss: 0.7140 - acc: 0.8188 - val_loss: 0.8916 - val_acc: 0.8034\n",
            "Epoch 205/300\n",
            " - 45s - loss: 0.7121 - acc: 0.8216 - val_loss: 0.8610 - val_acc: 0.8126\n",
            "Epoch 206/300\n",
            " - 44s - loss: 0.7072 - acc: 0.8230 - val_loss: 1.3515 - val_acc: 0.7347\n",
            "Epoch 207/300\n",
            " - 45s - loss: 0.7012 - acc: 0.8235 - val_loss: 0.9491 - val_acc: 0.7806\n",
            "Epoch 208/300\n",
            " - 45s - loss: 0.6989 - acc: 0.8264 - val_loss: 0.7705 - val_acc: 0.8159\n",
            "Epoch 209/300\n",
            " - 45s - loss: 0.7002 - acc: 0.8242 - val_loss: 1.0396 - val_acc: 0.7847\n",
            "Epoch 210/300\n",
            " - 45s - loss: 0.7038 - acc: 0.8237 - val_loss: 1.0181 - val_acc: 0.7861\n",
            "Epoch 211/300\n",
            " - 45s - loss: 0.7038 - acc: 0.8230 - val_loss: 0.8623 - val_acc: 0.8185\n",
            "Epoch 212/300\n",
            " - 45s - loss: 0.7046 - acc: 0.8245 - val_loss: 0.7211 - val_acc: 0.8229\n",
            "Epoch 213/300\n",
            " - 45s - loss: 0.7025 - acc: 0.8249 - val_loss: 0.8535 - val_acc: 0.8095\n",
            "Epoch 214/300\n",
            " - 45s - loss: 0.6862 - acc: 0.8254 - val_loss: 0.8462 - val_acc: 0.8096\n",
            "Epoch 215/300\n",
            " - 45s - loss: 0.7002 - acc: 0.8240 - val_loss: 1.1108 - val_acc: 0.7717\n",
            "Epoch 216/300\n",
            " - 45s - loss: 0.7020 - acc: 0.8226 - val_loss: 0.9523 - val_acc: 0.7898\n",
            "Epoch 217/300\n",
            " - 45s - loss: 0.7048 - acc: 0.8229 - val_loss: 0.7979 - val_acc: 0.8205\n",
            "Epoch 218/300\n",
            " - 44s - loss: 0.6998 - acc: 0.8233 - val_loss: 0.8297 - val_acc: 0.8052\n",
            "Epoch 219/300\n",
            " - 44s - loss: 0.6869 - acc: 0.8261 - val_loss: 0.7835 - val_acc: 0.8109\n",
            "Epoch 220/300\n",
            " - 45s - loss: 0.7005 - acc: 0.8261 - val_loss: 0.8135 - val_acc: 0.8195\n",
            "Epoch 221/300\n",
            " - 45s - loss: 0.6910 - acc: 0.8244 - val_loss: 0.8582 - val_acc: 0.8020\n",
            "Epoch 222/300\n",
            " - 45s - loss: 0.6934 - acc: 0.8270 - val_loss: 0.8626 - val_acc: 0.8156\n",
            "Epoch 223/300\n",
            " - 45s - loss: 0.6843 - acc: 0.8273 - val_loss: 0.9055 - val_acc: 0.7913\n",
            "Epoch 224/300\n",
            " - 45s - loss: 0.6891 - acc: 0.8257 - val_loss: 0.8820 - val_acc: 0.8001\n",
            "Epoch 225/300\n",
            " - 45s - loss: 0.6889 - acc: 0.8288 - val_loss: 1.0173 - val_acc: 0.7817\n",
            "Epoch 226/300\n",
            " - 45s - loss: 0.7079 - acc: 0.8235 - val_loss: 0.7825 - val_acc: 0.8148\n",
            "Epoch 227/300\n",
            " - 45s - loss: 0.6893 - acc: 0.8267 - val_loss: 0.8545 - val_acc: 0.7978\n",
            "Epoch 228/300\n",
            " - 45s - loss: 0.6947 - acc: 0.8256 - val_loss: 0.9385 - val_acc: 0.7843\n",
            "Epoch 229/300\n",
            " - 45s - loss: 0.6926 - acc: 0.8266 - val_loss: 0.8212 - val_acc: 0.8195\n",
            "Epoch 230/300\n",
            " - 46s - loss: 0.6969 - acc: 0.8257 - val_loss: 0.7712 - val_acc: 0.8258\n",
            "Epoch 231/300\n",
            " - 46s - loss: 0.6770 - acc: 0.8319 - val_loss: 0.8651 - val_acc: 0.8028\n",
            "Epoch 232/300\n",
            " - 45s - loss: 0.6818 - acc: 0.8288 - val_loss: 0.8187 - val_acc: 0.8040\n",
            "Epoch 233/300\n",
            " - 46s - loss: 0.6823 - acc: 0.8279 - val_loss: 0.8694 - val_acc: 0.8073\n",
            "Epoch 234/300\n",
            " - 45s - loss: 0.6795 - acc: 0.8296 - val_loss: 1.0340 - val_acc: 0.7921\n",
            "Epoch 235/300\n",
            " - 46s - loss: 0.6837 - acc: 0.8288 - val_loss: 0.9123 - val_acc: 0.8054\n",
            "Epoch 236/300\n",
            " - 46s - loss: 0.6855 - acc: 0.8273 - val_loss: 0.8902 - val_acc: 0.7826\n",
            "Epoch 237/300\n",
            " - 46s - loss: 0.6953 - acc: 0.8309 - val_loss: 0.9186 - val_acc: 0.7911\n",
            "Epoch 238/300\n",
            " - 47s - loss: 0.6739 - acc: 0.8310 - val_loss: 0.8487 - val_acc: 0.8056\n",
            "Epoch 239/300\n",
            " - 46s - loss: 0.6808 - acc: 0.8301 - val_loss: 1.0091 - val_acc: 0.7852\n",
            "Epoch 240/300\n",
            " - 47s - loss: 0.6749 - acc: 0.8292 - val_loss: 0.8822 - val_acc: 0.7943\n",
            "Epoch 241/300\n",
            " - 47s - loss: 0.6782 - acc: 0.8288 - val_loss: 0.8701 - val_acc: 0.8062\n",
            "Epoch 242/300\n",
            " - 47s - loss: 0.6829 - acc: 0.8301 - val_loss: 0.7812 - val_acc: 0.8179\n",
            "Epoch 243/300\n",
            " - 46s - loss: 0.6740 - acc: 0.8317 - val_loss: 0.9343 - val_acc: 0.7858\n",
            "Epoch 244/300\n",
            " - 46s - loss: 0.6652 - acc: 0.8324 - val_loss: 0.8615 - val_acc: 0.8223\n",
            "Epoch 245/300\n",
            " - 47s - loss: 0.6884 - acc: 0.8293 - val_loss: 0.7341 - val_acc: 0.8251\n",
            "Epoch 246/300\n",
            " - 46s - loss: 0.6767 - acc: 0.8304 - val_loss: 0.9081 - val_acc: 0.7901\n",
            "Epoch 247/300\n",
            " - 47s - loss: 0.6733 - acc: 0.8322 - val_loss: 1.0069 - val_acc: 0.7704\n",
            "Epoch 248/300\n",
            " - 47s - loss: 0.6681 - acc: 0.8357 - val_loss: 0.9395 - val_acc: 0.7971\n",
            "Epoch 249/300\n",
            " - 47s - loss: 0.6728 - acc: 0.8332 - val_loss: 0.7516 - val_acc: 0.8270\n",
            "Epoch 250/300\n",
            " - 46s - loss: 0.6766 - acc: 0.8307 - val_loss: 0.8375 - val_acc: 0.8260\n",
            "Epoch 251/300\n",
            " - 46s - loss: 0.6707 - acc: 0.8310 - val_loss: 0.7411 - val_acc: 0.8211\n",
            "Epoch 252/300\n",
            " - 46s - loss: 0.6816 - acc: 0.8305 - val_loss: 0.7278 - val_acc: 0.8364\n",
            "Epoch 253/300\n",
            " - 47s - loss: 0.6672 - acc: 0.8338 - val_loss: 0.8258 - val_acc: 0.8149\n",
            "Epoch 254/300\n",
            " - 46s - loss: 0.6741 - acc: 0.8320 - val_loss: 0.8707 - val_acc: 0.8079\n",
            "Epoch 255/300\n",
            " - 47s - loss: 0.6700 - acc: 0.8318 - val_loss: 0.8019 - val_acc: 0.8237\n",
            "Epoch 256/300\n",
            " - 47s - loss: 0.6605 - acc: 0.8364 - val_loss: 0.8903 - val_acc: 0.8111\n",
            "Epoch 257/300\n",
            " - 47s - loss: 0.6735 - acc: 0.8326 - val_loss: 0.8127 - val_acc: 0.8239\n",
            "Epoch 258/300\n",
            " - 46s - loss: 0.6653 - acc: 0.8349 - val_loss: 0.8836 - val_acc: 0.7976\n",
            "Epoch 259/300\n",
            " - 47s - loss: 0.6716 - acc: 0.8331 - val_loss: 0.7498 - val_acc: 0.8361\n",
            "Epoch 260/300\n",
            " - 46s - loss: 0.6688 - acc: 0.8358 - val_loss: 0.9653 - val_acc: 0.7979\n",
            "Epoch 261/300\n",
            " - 46s - loss: 0.6712 - acc: 0.8374 - val_loss: 0.8068 - val_acc: 0.8184\n",
            "Epoch 262/300\n",
            " - 46s - loss: 0.6634 - acc: 0.8354 - val_loss: 0.7699 - val_acc: 0.8215\n",
            "Epoch 263/300\n",
            " - 46s - loss: 0.6607 - acc: 0.8377 - val_loss: 0.7867 - val_acc: 0.8257\n",
            "Epoch 264/300\n",
            " - 47s - loss: 0.6605 - acc: 0.8361 - val_loss: 0.9005 - val_acc: 0.8164\n",
            "Epoch 265/300\n",
            " - 46s - loss: 0.6608 - acc: 0.8372 - val_loss: 0.8923 - val_acc: 0.8108\n",
            "Epoch 266/300\n",
            " - 46s - loss: 0.6689 - acc: 0.8346 - val_loss: 0.8011 - val_acc: 0.8232\n",
            "Epoch 267/300\n",
            " - 46s - loss: 0.6669 - acc: 0.8354 - val_loss: 0.8393 - val_acc: 0.8138\n",
            "Epoch 268/300\n",
            " - 46s - loss: 0.6599 - acc: 0.8373 - val_loss: 0.7983 - val_acc: 0.8143\n",
            "Epoch 269/300\n",
            " - 46s - loss: 0.6650 - acc: 0.8353 - val_loss: 0.8177 - val_acc: 0.8100\n",
            "Epoch 270/300\n",
            " - 46s - loss: 0.6564 - acc: 0.8379 - val_loss: 0.8411 - val_acc: 0.8087\n",
            "Epoch 271/300\n",
            " - 46s - loss: 0.6562 - acc: 0.8391 - val_loss: 0.7694 - val_acc: 0.8276\n",
            "Epoch 272/300\n",
            " - 46s - loss: 0.6569 - acc: 0.8348 - val_loss: 0.9102 - val_acc: 0.7906\n",
            "Epoch 273/300\n",
            " - 46s - loss: 0.6647 - acc: 0.8367 - val_loss: 0.8424 - val_acc: 0.8174\n",
            "Epoch 274/300\n",
            " - 46s - loss: 0.6555 - acc: 0.8373 - val_loss: 0.8071 - val_acc: 0.8183\n",
            "Epoch 275/300\n",
            " - 46s - loss: 0.6586 - acc: 0.8375 - val_loss: 0.9631 - val_acc: 0.7831\n",
            "Epoch 276/300\n",
            " - 46s - loss: 0.6589 - acc: 0.8351 - val_loss: 0.7624 - val_acc: 0.8157\n",
            "Epoch 277/300\n",
            " - 46s - loss: 0.6612 - acc: 0.8351 - val_loss: 1.0294 - val_acc: 0.7852\n",
            "Epoch 278/300\n",
            " - 46s - loss: 0.6601 - acc: 0.8380 - val_loss: 0.9222 - val_acc: 0.8019\n",
            "Epoch 279/300\n",
            " - 46s - loss: 0.6461 - acc: 0.8397 - val_loss: 0.8760 - val_acc: 0.8025\n",
            "Epoch 280/300\n",
            " - 47s - loss: 0.6530 - acc: 0.8369 - val_loss: 0.8241 - val_acc: 0.8211\n",
            "Epoch 281/300\n",
            " - 46s - loss: 0.6636 - acc: 0.8362 - val_loss: 0.7779 - val_acc: 0.8104\n",
            "Epoch 282/300\n",
            " - 46s - loss: 0.6519 - acc: 0.8385 - val_loss: 0.8647 - val_acc: 0.8200\n",
            "Epoch 283/300\n",
            " - 47s - loss: 0.6610 - acc: 0.8387 - val_loss: 0.7656 - val_acc: 0.8267\n",
            "Epoch 284/300\n",
            " - 46s - loss: 0.6453 - acc: 0.8409 - val_loss: 0.9030 - val_acc: 0.8022\n",
            "Epoch 285/300\n",
            " - 46s - loss: 0.6513 - acc: 0.8393 - val_loss: 0.9092 - val_acc: 0.8051\n",
            "Epoch 286/300\n",
            " - 46s - loss: 0.6536 - acc: 0.8369 - val_loss: 0.7916 - val_acc: 0.8218\n",
            "Epoch 287/300\n",
            " - 46s - loss: 0.6497 - acc: 0.8384 - val_loss: 0.7923 - val_acc: 0.8276\n",
            "Epoch 288/300\n",
            " - 46s - loss: 0.6539 - acc: 0.8387 - val_loss: 0.7551 - val_acc: 0.8303\n",
            "Epoch 289/300\n",
            " - 46s - loss: 0.6584 - acc: 0.8380 - val_loss: 1.0065 - val_acc: 0.7756\n",
            "Epoch 290/300\n",
            " - 46s - loss: 0.6477 - acc: 0.8389 - val_loss: 0.8897 - val_acc: 0.8090\n",
            "Epoch 291/300\n",
            " - 46s - loss: 0.6506 - acc: 0.8414 - val_loss: 0.9444 - val_acc: 0.8024\n",
            "Epoch 292/300\n",
            " - 45s - loss: 0.6488 - acc: 0.8405 - val_loss: 0.7562 - val_acc: 0.8215\n",
            "Epoch 293/300\n",
            " - 45s - loss: 0.6550 - acc: 0.8380 - val_loss: 0.7288 - val_acc: 0.8393\n",
            "Epoch 294/300\n",
            " - 45s - loss: 0.6452 - acc: 0.8402 - val_loss: 0.7653 - val_acc: 0.8169\n",
            "Epoch 295/300\n",
            " - 45s - loss: 0.6532 - acc: 0.8375 - val_loss: 0.8819 - val_acc: 0.8149\n",
            "Epoch 296/300\n",
            " - 45s - loss: 0.6472 - acc: 0.8410 - val_loss: 0.8040 - val_acc: 0.8341\n",
            "Epoch 297/300\n",
            " - 45s - loss: 0.6486 - acc: 0.8391 - val_loss: 0.7855 - val_acc: 0.8244\n",
            "Epoch 298/300\n",
            " - 45s - loss: 0.6425 - acc: 0.8411 - val_loss: 0.7005 - val_acc: 0.8316\n",
            "Epoch 299/300\n",
            " - 45s - loss: 0.6388 - acc: 0.8432 - val_loss: 0.9457 - val_acc: 0.8022\n",
            "Epoch 300/300\n",
            " - 45s - loss: 0.6408 - acc: 0.8447 - val_loss: 0.8388 - val_acc: 0.8103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e82861978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAvA2P_4YDSe",
        "colab_type": "code",
        "outputId": "ecdbd51e-c569-4e69-dff7-2dbba26a5510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Save model and weights\n",
        "save_dir = '/content/drive/My Drive/Colab Notebook'\n",
        "model5_name = 'keras_cifar10_trained_model_v6.h5' \n",
        "model_path = os.path.join(save_dir, model5_name)\n",
        "model5.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model5.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Colab Notebook/keras_cifar10_trained_model_v6.h5 \n",
            "10000/10000 [==============================] - 4s 402us/step\n",
            "Test loss: 0.8704138153076172\n",
            "Test accuracy: 0.8097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs15Bx96jonG",
        "colab_type": "code",
        "outputId": "758756d7-21f3-4729-a516-94a902350e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model5 = load_model(model_path)\n",
        "\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=0.0065, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy',\n",
        "               optimizer = opt,\n",
        "               metrics = ['acc'])\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "\n",
        "\n",
        "model5.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=64),\n",
        "                        epochs=300,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        callbacks = [StopTraining],\n",
        "                        workers=4,\n",
        "                        verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/300\n",
            " - 58s - loss: 0.6036 - acc: 0.8484 - val_loss: 0.7596 - val_acc: 0.8291\n",
            "Epoch 2/300\n",
            " - 45s - loss: 0.5894 - acc: 0.8522 - val_loss: 0.8661 - val_acc: 0.8089\n",
            "Epoch 3/300\n",
            " - 46s - loss: 0.5864 - acc: 0.8538 - val_loss: 0.8437 - val_acc: 0.8108\n",
            "Epoch 4/300\n",
            " - 46s - loss: 0.5870 - acc: 0.8552 - val_loss: 0.7108 - val_acc: 0.8367\n",
            "Epoch 5/300\n",
            " - 46s - loss: 0.5841 - acc: 0.8527 - val_loss: 0.7057 - val_acc: 0.8405\n",
            "Epoch 6/300\n",
            " - 46s - loss: 0.5821 - acc: 0.8538 - val_loss: 0.7523 - val_acc: 0.8301\n",
            "Epoch 7/300\n",
            " - 46s - loss: 0.5840 - acc: 0.8525 - val_loss: 0.7844 - val_acc: 0.8241\n",
            "Epoch 8/300\n",
            " - 46s - loss: 0.5797 - acc: 0.8529 - val_loss: 0.6902 - val_acc: 0.8405\n",
            "Epoch 9/300\n",
            " - 46s - loss: 0.5714 - acc: 0.8571 - val_loss: 0.7965 - val_acc: 0.8216\n",
            "Epoch 10/300\n",
            " - 46s - loss: 0.5819 - acc: 0.8554 - val_loss: 0.7765 - val_acc: 0.8250\n",
            "Epoch 11/300\n",
            " - 46s - loss: 0.5792 - acc: 0.8554 - val_loss: 0.6840 - val_acc: 0.8455\n",
            "Epoch 12/300\n",
            " - 46s - loss: 0.5720 - acc: 0.8565 - val_loss: 0.7859 - val_acc: 0.8292\n",
            "Epoch 13/300\n",
            " - 46s - loss: 0.5790 - acc: 0.8546 - val_loss: 0.7681 - val_acc: 0.8320\n",
            "Epoch 14/300\n",
            " - 46s - loss: 0.5688 - acc: 0.8587 - val_loss: 0.7363 - val_acc: 0.8318\n",
            "Epoch 15/300\n",
            " - 46s - loss: 0.5788 - acc: 0.8537 - val_loss: 0.8005 - val_acc: 0.8287\n",
            "Epoch 16/300\n",
            " - 46s - loss: 0.5748 - acc: 0.8558 - val_loss: 0.6995 - val_acc: 0.8447\n",
            "Epoch 17/300\n",
            " - 47s - loss: 0.5751 - acc: 0.8558 - val_loss: 0.7038 - val_acc: 0.8383\n",
            "Epoch 18/300\n",
            " - 46s - loss: 0.5703 - acc: 0.8563 - val_loss: 0.8486 - val_acc: 0.8246\n",
            "Epoch 19/300\n",
            " - 46s - loss: 0.5745 - acc: 0.8571 - val_loss: 0.8121 - val_acc: 0.8299\n",
            "Epoch 20/300\n",
            " - 46s - loss: 0.5767 - acc: 0.8537 - val_loss: 0.7093 - val_acc: 0.8383\n",
            "Epoch 21/300\n",
            " - 46s - loss: 0.5767 - acc: 0.8541 - val_loss: 0.8495 - val_acc: 0.8209\n",
            "Epoch 22/300\n",
            " - 46s - loss: 0.5679 - acc: 0.8577 - val_loss: 0.6960 - val_acc: 0.8358\n",
            "Epoch 23/300\n",
            " - 46s - loss: 0.5718 - acc: 0.8546 - val_loss: 0.7728 - val_acc: 0.8215\n",
            "Epoch 24/300\n",
            " - 46s - loss: 0.5682 - acc: 0.8579 - val_loss: 0.8508 - val_acc: 0.8181\n",
            "Epoch 25/300\n",
            " - 46s - loss: 0.5659 - acc: 0.8573 - val_loss: 0.7327 - val_acc: 0.8270\n",
            "Epoch 26/300\n",
            " - 45s - loss: 0.5731 - acc: 0.8558 - val_loss: 0.7402 - val_acc: 0.8406\n",
            "Epoch 27/300\n",
            " - 45s - loss: 0.5712 - acc: 0.8563 - val_loss: 0.8108 - val_acc: 0.8231\n",
            "Epoch 28/300\n",
            " - 45s - loss: 0.5690 - acc: 0.8562 - val_loss: 0.7142 - val_acc: 0.8355\n",
            "Epoch 29/300\n",
            " - 46s - loss: 0.5632 - acc: 0.8608 - val_loss: 0.7702 - val_acc: 0.8329\n",
            "Epoch 30/300\n",
            " - 46s - loss: 0.5720 - acc: 0.8593 - val_loss: 0.8122 - val_acc: 0.8261\n",
            "Epoch 31/300\n",
            " - 45s - loss: 0.5657 - acc: 0.8575 - val_loss: 0.7154 - val_acc: 0.8463\n",
            "Epoch 32/300\n",
            " - 46s - loss: 0.5646 - acc: 0.8568 - val_loss: 0.7488 - val_acc: 0.8299\n",
            "Epoch 33/300\n",
            " - 45s - loss: 0.5725 - acc: 0.8570 - val_loss: 0.7965 - val_acc: 0.8238\n",
            "Epoch 34/300\n",
            " - 45s - loss: 0.5693 - acc: 0.8580 - val_loss: 0.8175 - val_acc: 0.8222\n",
            "Epoch 35/300\n",
            " - 45s - loss: 0.5619 - acc: 0.8598 - val_loss: 0.8012 - val_acc: 0.8272\n",
            "Epoch 36/300\n",
            " - 45s - loss: 0.5616 - acc: 0.8597 - val_loss: 0.7801 - val_acc: 0.8321\n",
            "Epoch 37/300\n",
            " - 46s - loss: 0.5674 - acc: 0.8596 - val_loss: 0.9356 - val_acc: 0.8073\n",
            "Epoch 38/300\n",
            " - 45s - loss: 0.5657 - acc: 0.8586 - val_loss: 0.7373 - val_acc: 0.8266\n",
            "Epoch 39/300\n",
            " - 46s - loss: 0.5634 - acc: 0.8601 - val_loss: 0.7146 - val_acc: 0.8315\n",
            "Epoch 40/300\n",
            " - 46s - loss: 0.5638 - acc: 0.8581 - val_loss: 0.6930 - val_acc: 0.8370\n",
            "Epoch 41/300\n",
            " - 46s - loss: 0.5650 - acc: 0.8600 - val_loss: 0.8153 - val_acc: 0.8252\n",
            "Epoch 42/300\n",
            " - 46s - loss: 0.5622 - acc: 0.8604 - val_loss: 0.9844 - val_acc: 0.8108\n",
            "Epoch 43/300\n",
            " - 46s - loss: 0.5606 - acc: 0.8591 - val_loss: 0.7784 - val_acc: 0.8295\n",
            "Epoch 44/300\n",
            " - 46s - loss: 0.5594 - acc: 0.8570 - val_loss: 0.8057 - val_acc: 0.8261\n",
            "Epoch 45/300\n",
            " - 46s - loss: 0.5649 - acc: 0.8586 - val_loss: 0.9226 - val_acc: 0.7949\n",
            "Epoch 46/300\n",
            " - 45s - loss: 0.5600 - acc: 0.8591 - val_loss: 0.7095 - val_acc: 0.8426\n",
            "Epoch 47/300\n",
            " - 45s - loss: 0.5578 - acc: 0.8586 - val_loss: 0.7346 - val_acc: 0.8428\n",
            "Epoch 48/300\n",
            " - 45s - loss: 0.5654 - acc: 0.8590 - val_loss: 0.7788 - val_acc: 0.8332\n",
            "Epoch 49/300\n",
            " - 45s - loss: 0.5615 - acc: 0.8590 - val_loss: 0.7387 - val_acc: 0.8348\n",
            "Epoch 50/300\n",
            " - 45s - loss: 0.5603 - acc: 0.8582 - val_loss: 0.9538 - val_acc: 0.8103\n",
            "Epoch 51/300\n",
            " - 45s - loss: 0.5625 - acc: 0.8594 - val_loss: 0.8325 - val_acc: 0.8153\n",
            "Epoch 52/300\n",
            " - 45s - loss: 0.5663 - acc: 0.8563 - val_loss: 0.7223 - val_acc: 0.8400\n",
            "Epoch 53/300\n",
            " - 45s - loss: 0.5550 - acc: 0.8590 - val_loss: 0.8843 - val_acc: 0.8154\n",
            "Epoch 54/300\n",
            " - 45s - loss: 0.5657 - acc: 0.8599 - val_loss: 0.8067 - val_acc: 0.8312\n",
            "Epoch 55/300\n",
            " - 45s - loss: 0.5558 - acc: 0.8622 - val_loss: 0.8890 - val_acc: 0.8138\n",
            "Epoch 56/300\n",
            " - 45s - loss: 0.5528 - acc: 0.8617 - val_loss: 0.8302 - val_acc: 0.8300\n",
            "Epoch 57/300\n",
            " - 46s - loss: 0.5636 - acc: 0.8599 - val_loss: 0.7483 - val_acc: 0.8361\n",
            "Epoch 58/300\n",
            " - 45s - loss: 0.5534 - acc: 0.8617 - val_loss: 0.7734 - val_acc: 0.8369\n",
            "Epoch 59/300\n",
            " - 46s - loss: 0.5551 - acc: 0.8610 - val_loss: 0.8230 - val_acc: 0.8241\n",
            "Epoch 60/300\n",
            " - 46s - loss: 0.5497 - acc: 0.8631 - val_loss: 0.9538 - val_acc: 0.8049\n",
            "Epoch 61/300\n",
            " - 45s - loss: 0.5613 - acc: 0.8582 - val_loss: 0.7390 - val_acc: 0.8366\n",
            "Epoch 62/300\n",
            " - 45s - loss: 0.5603 - acc: 0.8610 - val_loss: 0.7525 - val_acc: 0.8391\n",
            "Epoch 63/300\n",
            " - 45s - loss: 0.5523 - acc: 0.8623 - val_loss: 0.7228 - val_acc: 0.8359\n",
            "Epoch 64/300\n",
            " - 45s - loss: 0.5427 - acc: 0.8629 - val_loss: 0.7535 - val_acc: 0.8398\n",
            "Epoch 65/300\n",
            " - 45s - loss: 0.5561 - acc: 0.8605 - val_loss: 0.8267 - val_acc: 0.8202\n",
            "Epoch 66/300\n",
            " - 45s - loss: 0.5548 - acc: 0.8612 - val_loss: 0.8301 - val_acc: 0.8279\n",
            "Epoch 67/300\n",
            " - 45s - loss: 0.5603 - acc: 0.8597 - val_loss: 0.6330 - val_acc: 0.8509\n",
            "\n",
            "Reached 85% on validation set!!! So stop training !!!\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e7e052160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lX36s5ikTaS",
        "colab_type": "code",
        "outputId": "3e0b0f40-fa30-47c2-aa1b-f8f6a84f6faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Save model and weights\n",
        "save_dir = '/content/drive/My Drive/Colab Notebook'\n",
        "model5_name = 'keras_cifar10_trained_model_v6.h5' \n",
        "model_path = os.path.join(save_dir, model5_name)\n",
        "model5.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model5.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Colab Notebook/keras_cifar10_trained_model_v6.h5 \n",
            "10000/10000 [==============================] - 4s 411us/step\n",
            "Test loss: 0.6340710081577301\n",
            "Test accuracy: 0.8468\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}